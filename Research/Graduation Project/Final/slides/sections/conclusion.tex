\section{Conclusion}

\begin{frame}{Main Conclusion}
    \begin{block}{Finding}
        \textbf{A general correlation between LMC Complexity and Generalization Capability cannot be confirmed.}
    \end{block}

    \vspace{0.5cm}
    \begin{itemize}
        \item While statistical significance was found in aggregated data, the relationship is inconsistent across individual benchmarks.
        \item The results suggest that LMC Complexity, in its current form, is not a reliable universal predictor of model performance.
    \end{itemize}
\end{frame}

\begin{frame}{Evidence Supporting the Hypothesis}
    \begin{itemize}
        \item \textbf{Aggregated Significance}:
        \begin{itemize}
            \item The aggregated dataset (\textbf{All}) showed a statistically significant positive correlation ($p < 0.05$).
        \end{itemize}
        \item \textbf{Positive Bias}:
        \begin{itemize}
            \item Positive correlations (e.g., MMLU) were stronger and more significant than negative ones.
        \end{itemize}
        \item \textbf{Parameter Relation}:
        \begin{itemize}
            \item Complexity tends to increase with parameter count, which is a known predictor of performance.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Evidence Against the Hypothesis}
    \begin{itemize}
        \item \textbf{Outlier Dependence}:
        \begin{itemize}
            \item Positive trends were heavily driven by specific outliers (e.g., GPT-OSS family).
            \item Removing outliers often reduced correlations to near zero.
        \end{itemize}
        \item \textbf{Inconsistency}:
        \begin{itemize}
            \item Different benchmarks yielded contradictory results (Positive vs. Negative correlations).
            \item Regression shapes varied widely (Constant, Linear, Dual-trend).
        \end{itemize}
        \item \textbf{Predictive Power}:
        \begin{itemize}
            \item Low $R^2$ values compared to the control (Parameter Count).
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Future Work}
    \begin{enumerate}
        \item \textbf{Controlled Training \& Optimization}:
        \begin{itemize}
            \item Train models from scratch to compare Test Loss vs. Complexity directly (Intra-Model Analysis).
            \item Eliminates the noise and inconsistency of public benchmarks.
            \item Investigate if maximizing LMC complexity improves performance.
        \end{itemize}
        \item \textbf{Filtering Refinement}:
        \begin{itemize}
            \item Explore less aggressive filtering (e.g., 30 $\sigma$) and the complexity spike at 0.125 $\sigma$.
        \end{itemize}
        \item \textbf{Outlier Investigation}:
        \begin{itemize}
            \item Investigate why the GPT-OSS family is an outlier for LMC complexity.
        \end{itemize}
        \item \textbf{Alternative Hypothesis}:
        \begin{itemize}
            \item Complexity might measure "distance to performance ceiling" (Early Stopping Criterion).
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \centering
    \Huge \textbf{Thank You!}
    
    \vspace{1cm}
    \large Questions?
\end{frame}
