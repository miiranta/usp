\section{Methodology}

\begin{frame}{Experimental Setup}
    \begin{itemize}
        \item \textbf{Hardware Constraints}:
        \begin{itemize}
            \item \textbf{RAM}: 512GB DDR4 (Critical for loading large models).
            \item \textbf{GPU}: NVIDIA Quadro P5000 (16GB).
            \item \textbf{CPU}: 2x Intel Xeon Gold 6130 (64 threads).
        \end{itemize}
        \item \textbf{Software Stack}:
        \begin{itemize}
            \item Python 3.12, PyTorch 2.8, Transformers 4.56.
            \item Models loaded in \textbf{Main Memory (CPU)} cast to \texttt{float32}.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Model Selection Strategy}
    \begin{itemize}
        \item \textbf{Source}: Hugging Face (Open Weights) \cite{Wolf2019, HuggingFaceMain}.
        \item \textbf{Criteria}:
        \begin{itemize}
            \item Must be available on Hugging Face (official company account).
            \item Transformer-based language model.
            \item Open weights (including gated access).
            \item Text-only (no multimodal inputs).
            \item Base model (no fine-tunes).
            \item Parameter count $< 150$ Billion (Hardware limit).
            \item Supported by \texttt{AutoModel} utility.
            \item Has benchmark results available.
        \end{itemize}
        \item \textbf{Selected Models (35 Total)}:
        \begin{itemize}
            \item \textbf{Meta}: Llama 2, 3, 3.1, 3.2, 4 (Scout).
            \item \textbf{Google}: Gemma 1, 2, 3, RecurrentGemma.
            \item \textbf{Microsoft}: Phi-1, 1.5, 2, 4 (Mini/Reasoning).
            \item \textbf{OpenAI}: GPT-2 (Small to XL), GPT-OSS (120B, 20B).
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Inference Capability: Benchmarks}
    \begin{itemize}
        \item \textbf{Why Benchmarks?}:
        \begin{itemize}
            \item Proxies for Test Loss (Performance) \cite{Owen2024}.
            \item Training from scratch/Test loss unavailable for all models.
        \end{itemize}
        \item \textbf{Selection Criteria}:
        \begin{itemize}
            \item \textbf{Relevance}: Widely recognized (e.g., MMLU).
            \item \textbf{Generality}: Covers range of tasks.
            \item \textbf{Availability}: Results publicly available.
        \end{itemize}
        \item \textbf{Selected Benchmarks}:
        \begin{itemize}
            \item \textbf{MMLU}: 57 tasks, STEM/Humanities. Standard for LLMs \cite{Hendrycks2020}.
            \item \textbf{MMLU-Pro}: Enhanced MMLU, harder reasoning \cite{MMPro2024}.
            \item \textbf{OpenLLM}: Aggregated score of multiple datasets \cite{Myrzakhan2024}.
            \item \textbf{LMArena}: Crowdsourced Elo ratings based on human preference \cite{Chiang2024}.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Benchmark Availability}
    \begin{figure}
        \centering
        \includegraphics[width=0.8\linewidth]{../img/benchmark_availability.png}
        \caption{Availability of benchmark results for the selected models.}
    \end{figure}
    \begin{itemize}
        \item Not all models have results for all benchmarks.
        \item Data collected from official pages, papers, and leaderboards.
    \end{itemize}
\end{frame}

\begin{frame}{LMC Statistical Complexity}
    \begin{block}{Definition}
        \[ C_{LMC} = H \times D \]
    \end{block}
    \begin{itemize}
        \item \textbf{Disequilibrium ($D$)}:
        \begin{itemize}
            \item Measures distance from uniform distribution ("Order").
            \item $D = \sum_{i=1}^{n} (p_i - \frac{1}{n})^2$
        \end{itemize}
        \item \textbf{Shannon Entropy ($H$)}:
        \begin{itemize}
            \item Measures uncertainty or randomness.
            \item $H = -K \sum_{i=1}^{n} p_i \log p_i$
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{LMC Statistical Complexity Extraction}
    \begin{enumerate}
        \item \textbf{Weight Extraction}:
        \begin{itemize}
            \item Flatten tensors from \texttt{named\_parameters()}.
            \item Categorize: Bias, Norm, Embedding, Other.
            \item Tested combinations: Power set of categories (15 combinations).
        \end{itemize}
        \item \textbf{Filtering}:
        \begin{itemize}
            \item Remove outliers.
            \item Range: $\mu \pm \sigma_{\text{filter}} \cdot \sigma$.
            \item Tested $\sigma_{\text{filter}} \in \{0.125, \dots, 20, 40(\text{unfiltered})\}$.
        \end{itemize}
        \item \textbf{Discretization (Histogram)}:
        \begin{itemize}
            \item \textbf{Freedman-Diaconis Rule} \cite{FreedmanDiaconis1981}: $h = \frac{2 \times IQR}{N^{1/3}}$.
            \item Adapts to distribution spread and sample size ($N$).
        \end{itemize}
        \item \textbf{Calculate LMC}:
        \begin{itemize}
            \item Compute $C_{LMC} = H \times D$ using the histogram probabilities.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}{Analysis Dimensions: Dataset}
    \begin{itemize}
        \item \textbf{Dataset Construction}:
        \begin{itemize}
            \item \textbf{Models}: 35 Selected Models.
            \item \textbf{Weight Combinations}: 15 (Power set of Bias, Norm, Embedding, Other).
            \item \textbf{Filtering Settings}: 11 ($\sigma_{\text{filter}}$ values).
            \item \textbf{Total Data Points}: $35 \times 15 \times 11 \approx 5775$.
        \end{itemize}
        \item \textbf{Tuple Structure}:
        \begin{itemize}
            \item (Model, Params, Weight-Type, Filter, Complexity, Bins, Benchmarks).
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Analysis Dimensions: Statistical Tools}
    \begin{itemize}
        \item \textbf{Correlation Analysis}:
        \begin{itemize}
            \item \textbf{Pearson Correlation ($r$)}: Measure linear relationship.
            \item \textbf{T-tests}: Determine statistical significance ($p < 0.05$).
        \end{itemize}
        \item \textbf{Regression Analysis}:
        \begin{itemize}
            \item \textbf{Linear Regression}: $y = ax + b$.
            \item \textbf{Free Regression}: Curve fitting (Linear, Quadratic, Exponential, Logarithmic, Power).
            \item \textbf{$R^2$ Score}: Measure goodness of fit.
        \end{itemize}
    \end{itemize}
\end{frame}
