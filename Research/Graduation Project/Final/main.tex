\documentclass[
12pt,
openany,
twoside,
a4paper,
englishwr,
english
]{ppgca}

%\usepackage[style=ieee,backend=biber,citestyle=numeric]{biblatex}
\usepackage{csquotes}
\usepackage{enumitem}
\usepackage{float}
\usepackage{tcolorbox}
\usepackage[left=2.5cm,right=2.5cm,top=3cm,bottom=2cm]{geometry}

%\addbibresource{refs.bib}

% User's custom commands
\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}

% Info
\title{Comparing the LMC Complexity of Neural Networks with their Generalization Capability}
\author{Lucas Miranda Mendonça Rezende}
\local{Ribeirão Preto--SP}
\data{2025}
\orientador{Ph.D. Luiz Otavio Murta Junior}
\tipotrabalho{Dissertation}

\preambulo{Monograph presented to the Faculty of Philosophy, Sciences and Letters of Ribeirão Preto (FFCLRP) from the University of São Paulo (USP), as part of the requirements to hold the Bachelor of Science degree.}

\makeindex

\begin{document}

\selectlanguage{english}
\renewcommand{\contentsname}{Table of Contents}
\renewcommand{\bibname}{References}

\imprimircapa
\imprimirfolhaderosto

% Abstract
\begin{resumo}[Abstract]
    Current scaling laws suggest that model performance improves with increased parameters, dataset size, and compute, but these improvements follow power laws requiring exponential resources for constant gains. Understanding the learning process through alternative metrics could enable architectural improvements and optimization during training. This work investigates whether LMC statistical complexity, a measure combining Shannon entropy and disequilibrium of weight distributions, correlates with neural network generalization capability. We analyzed 35 open-weight transformer-based language models from Meta, Google, Microsoft, and OpenAI, ranging from 117 million to 150 billion parameters, computing LMC complexity across different weight-type combinations (bias, norm, embedding, other) and filtering settings. Model performance was evaluated using four benchmarks: MMLU, MMLU-Pro, OpenLLM, and LMArena. Results revealed that, despite statistical significance in aggregated analyses and specific benchmarks like MMLU, a general correlation between LMC complexity and benchmark performance cannot be confirmed. The observed positive trends were often driven by outliers or showed inconsistent patterns, suggesting that LMC complexity might not be a reliable predictor of generalization capability in the studied context.
\end{resumo}

\pdfbookmark[0]{\contentsname}{toc}
\tableofcontents*
\cleardoublepage

\textual

\input{sections/introduction}
\input{sections/methods}
\input{sections/results}
\input{sections/conclusion}

\bibliographystyle{ieeetr}
\bibliography{refs}

\end{document}