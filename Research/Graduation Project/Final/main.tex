\documentclass[12pt,a4paper]{article}
\usepackage{a4wide}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[style=ieee,backend=biber,citestyle=numeric]{biblatex}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{geometry} 
\usepackage{graphicx}
\usepackage{float}
\usepackage{tcolorbox}

\addbibresource{refs.bib}

\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}

\newcommand{\subsubsubsection}[1]{%
    \refstepcounter{paragraph}%
    \addcontentsline{toc}{paragraph}{\theparagraph\quad #1}
    \vspace{0.6em}%
    \noindent\textbf{\theparagraph\quad #1}\par
    \vspace{0.2em}%
}

\title{
    {\small 
        Universidade de São Paulo\\
        Faculdade de Filosofia, Ciências e Letras de Ribeirão Preto\\
        Departamento de Computação e Matemática\\[1cm]
    }

    \vspace*{\fill}
    Comparing the LMC Complexity of Neural Networks with their Inference Capability
    \vspace*{\fill}
}
\author{
    Author: Lucas Miranda Mendonça Rezende\\ 
    Supervisor: Ph.D. Luiz Otavio Murta Junior
}
\date{\today}

\setlength{\parskip}{.5ex}

\begin{document}
    \pagenumbering{gobble}
        \maketitle

    \newpage
    \pagenumbering{arabic}
        \vspace*{\fill}
            \begin{abstract}
                Current scaling laws suggest that model performance improves with increased parameters, dataset size, and compute, but these improvements follow power laws requiring exponential resources for constant gains. Understanding the learning process through alternative metrics could enable architectural improvements and optimization during training. This work investigates whether LMC statistical complexity, a measure combining Shannon entropy and disequilibrium of weight distributions, correlates with neural network inference capability. We analyzed 35 open-weight transformer-based language models from Meta, Google, Microsoft, and OpenAI, ranging from 117 million to 150 billion parameters, computing LMC complexity across different weight-type combinations (bias, norm, embedding, other) and filtering settings. Model performance was evaluated using four benchmarks: MMLU, MMLU-Pro, OpenLLM, and LMArena. Results showed no statistically significant general correlation between LMC complexity and benchmark performance while individual benchmarks exhibited mixed results.
            \end{abstract}
        \vspace*{\fill}

    \newpage
        \tableofcontents

    \newpage
    \input{sections/introduction}

    \newpage
    \input{sections/methods}

    \newpage
    \input{sections/results}

    \newpage
    \input{sections/conclusion}

    \newpage
    \printbibliography

\end{document}