@article{Vaswani2017,
  title={Attention Is All You Need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  journal={arXiv preprint arXiv:1706.03762},
  year={2017},
  url={https://arxiv.org/abs/1706.03762}
}

@article{Kaplan2020,
  title={Scaling Laws for Neural Language Models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020},
  url={https://arxiv.org/abs/2001.08361}
}

@article{Brown2020,
  title={Language Models are Few-Shot Learners},
  author={Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and others},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020},
  url={https://arxiv.org/abs/2005.14165}
}

@article{LopezRuiz1995,
  title={A Statistical Measure of Complexity},
  author={Lopez-Ruiz, Ricardo and Mancini, Hector and Calbet, Xavier},
  journal={Physics Letters A},
  volume={209},
  number={5-6},
  pages={321--326},
  year={1995}
}

@article{ChatGPTAdoption2022,
  author={Trust, T. and Whalen, J. and Mouza, C.},
  title={Editorial: ChatGPT: Challenges, Opportunities, and Implications for Teacher Education},
  journal={Contemporary Issues in Technology and Teacher Education},
  volume={23},
  number={1},
  pages={1--23},
  year={2023},
  publisher={Society for Information Technology \& Teacher Education},
  address={Waynesville, NC USA},
  url={https://www.learntechlib.org/primary/p/222408/}
}

@article{Radford2018,
  title={Improving Language Understanding by Generative Pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  publisher={OpenAI},
  url={https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf},
  year={2018}
}

@article{Radford2019,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  publisher={OpenAI},
  url={https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf},
  year={2019}
}

@article{Wolf2019,
  title={HuggingFace's Transformers: State-of-the-art Natural Language Processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and others},
  journal={arXiv preprint arXiv:1910.03771},
  year={2019},
  url={https://arxiv.org/abs/1910.03771}
}

@article{Hendrycks2020,
  title={Measuring Massive Multitask Language Understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2009.03300},
  year={2020},
  url={https://arxiv.org/abs/2009.03300}
}

@article{Myrzakhan2024,
  title={Open-LLM-Leaderboard: From Multi-choice to Open-style Questions for LLMs Evaluation, Benchmark, and Arena},
  author={Myrzakhan, Aidar and Bsharat, Sondos Mahmoud and Shen, Zhiqiang},
  journal={arXiv preprint arXiv:2406.07545},
  year={2024},
  url={https://arxiv.org/abs/2406.07545}
}

@article{Chiang2024,
  title={Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference},
  author={Chiang, Wei-Lin and Zheng, Lianmin and Sheng, Ying and Angelopoulos, Anastasios Nikolas and Li, Tianle and Li, Dacheng and Zhang, Hao and Zhu, Banghua and Jordan, Michael and others},
  journal={arXiv preprint arXiv:2403.04132},
  year={2024},
  url={https://arxiv.org/abs/2403.04132}
}

@article{Magar2022,
  title={Data Contamination: From Memorization to Exploitation},
  author={Magar, Inbal and Schwartz, Roy},
  journal={arXiv preprint arXiv:2203.08242},
  year={2022},
  url={https://arxiv.org/abs/2203.08242}
}

@article{Owen2024,
  title={How predictable is language model benchmark performance?},
  author={Owen, David},
  journal={arXiv preprint arXiv:2401.04757},
  year={2024},
  url={https://arxiv.org/abs/2401.04757}
}

@article{Knuth2006,
  title={Optimal Data-Based Binning for Histograms},
  author={Knuth, Kevin H.},
  journal={arXiv preprint arXiv:physics/0605197},
  year={2006},
  url={https://arxiv.org/abs/physics/0605197}
}

@article{Sturges1926,
  title={The Choice of a Class Interval},
  author={Sturges, Herbert A.},
  journal={Journal of the American Statistical Association},
  volume={21},
  number={153},
  pages={65--66},
  year={1926},
  url={https://www.jstor.org/stable/2965501}
}

@article{RiceRule,
  author       = {Moral De La Rubia, José},
  title        = {Rice University Rule to Determine the Number of Bins},
  journal      = {Open Journal of Statistics},
  year         = {2024},
  doi          = {10.4236/ojs.2024.141006},
  url={https://www.scirp.org/pdf/ojs_2024022914191399.pdf},
}

@article{FreedmanDiaconis1981,
  title={On the Histogram as a Density Estimator: {L$_2$} Theory},
  author={Freedman, David and Diaconis, Persi},
  journal={Zeitschrift f\"{u}r Wahrscheinlichkeitstheorie und Verwandte Gebiete},
  volume={57},
  pages={453--476},
  year={1981},
  url={https://link.springer.com/content/pdf/10.1007/BF01025868.pdf}
}

@article{Phi1.5,
  title={Textbooks Are All You Need II: phi-1.5 technical report},
  author={Li, Yuanzhi and Bubeck, Sébastien and Eldan, Ronen and Del Giorno, Allie and Gunasekar, Suriya and Lee, Yin Tat},
  journal={arXiv preprint arXiv:2309.05463},
  year={2023},
  url={https://arxiv.org/abs/2309.05463}
}

@article{MMPro2024,
  title={MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark},
  author={Wang, Yubo and Ma, Xueguang and Zhang, Ge and Ni, Yuansheng and Chandra, Abhranil and Guo, Shiguang and Ren, Weiming and Arulraj, Aaran and He, Xuan and others},
  journal={arXiv preprint arXiv:2406.01574},
  year={2024},
  url={https://arxiv.org/abs/2406.01574}
}

@misc{HuggingFaceResearchGate,
  title={Hugging Face Revolutionizing AI and NLP},
  author={Urmila R. Pol and Parashuram S. Vadar and Tejashree T. Moharekar},
  howpublished={\url{https://www.researchgate.net/profile/Tejashree-Moharekar/publication/383497950}},
  year={2024}
}

@misc{HuggingFaceMain,
  title={Hugging Face -- The AI community building the future},
  howpublished={\url{https://huggingface.co/}},
  year={2024}
}

@misc{Phi2Blog,
  title={Phi-2: The Surprising Power of Small Language Models},
  author={Javaheripi, Mojan and Bubeck, S{\'e}bastien},
  howpublished={\url{https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/}},
  year={2023}
}

@misc{OpenAIOpenModels,
  title={Open Models},
  author={OpenAI},
  howpublished={\url{https://openai.com/open-models/}},
  year={2024}
}

@misc{Ollama,
  title={Ollama -- Get up and running with large language models locally},
  howpublished={\url{https://ollama.com/}},
  year={2024}
}

@misc{RankedAGI,
  title={Ranked AGI},
  howpublished={\url{https://rankedagi.com/}},
  year={2024}
}

@misc{LLMExplorer,
  title={LLM Explorer -- Compare Language Models},
  howpublished={\url{https://llm-explorer.com/}},
  year={2024}
}

% Meta Models on Hugging Face
@misc{MetaLlama4Scout,
  title={Meta Llama 4 Scout 17B 16E},
  howpublished={\url{https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E}},
  year={2024}
}

@misc{MetaLlama3.2-3B,
  title={Meta Llama 3.2 3B},
  howpublished={\url{https://huggingface.co/meta-llama/Llama-3.2-3B}},
  year={2024}
}

@misc{MetaLlama3.2-1B,
  title={Meta Llama 3.2 1B},
  howpublished={\url{https://huggingface.co/meta-llama/Llama-3.2-1B}},
  year={2024}
}

@misc{MetaLlama3.1-70B,
  title={Meta Llama 3.1 70B},
  howpublished={\url{https://huggingface.co/meta-llama/Llama-3.1-70B}},
  year={2024}
}

@misc{MetaLlama3.1-8B,
  title={Meta Llama 3.1 8B},
  howpublished={\url{https://huggingface.co/meta-llama/Llama-3.1-8B}},
  year={2024}
}

@misc{MetaLlama3-70B,
  title={Meta Llama 3 70B},
  howpublished={\url{https://huggingface.co/meta-llama/Meta-Llama-3-70B}},
  year={2024}
}

@misc{MetaLlama3-8B,
  title={Meta Llama 3 8B},
  howpublished={\url{https://huggingface.co/meta-llama/Meta-Llama-3-8B}},
  year={2024}
}

@misc{MetaLlama2-70b,
  title={Meta Llama 2 70B HF},
  howpublished={\url{https://huggingface.co/meta-llama/Llama-2-70b-hf}},
  year={2024}
}

@misc{MetaLlama2-13b,
  title={Meta Llama 2 13B HF},
  howpublished={\url{https://huggingface.co/meta-llama/Llama-2-13b-hf}},
  year={2024}
}

@misc{MetaLlama2-7b,
  title={Meta Llama 2 7B HF},
  howpublished={\url{https://huggingface.co/meta-llama/Llama-2-7b-hf}},
  year={2024}
}

% Google Models on Hugging Face
@misc{GoogleGemma3-27b,
  title={Google Gemma 3 27B PT},
  howpublished={\url{https://huggingface.co/google/gemma-3-27b-pt}},
  year={2024}
}

@misc{GoogleGemma3-12b,
  title={Google Gemma 3 12B PT},
  howpublished={\url{https://huggingface.co/google/gemma-3-12b-pt}},
  year={2024}
}

@misc{GoogleGemma3-4b,
  title={Google Gemma 3 4B PT},
  howpublished={\url{https://huggingface.co/google/gemma-3-4b-pt}},
  year={2024}
}

@misc{GoogleGemma3-1b,
  title={Google Gemma 3 1B PT},
  howpublished={\url{https://huggingface.co/google/gemma-3-1b-pt}},
  year={2024}
}

@misc{GoogleGemma2-27b,
  title={Google Gemma 2 27B},
  howpublished={\url{https://huggingface.co/google/gemma-2-27b}},
  year={2024}
}

@misc{GoogleGemma2-9b,
  title={Google Gemma 2 9B},
  howpublished={\url{https://huggingface.co/google/gemma-2-9b}},
  year={2024}
}

@misc{GoogleGemma2-2b,
  title={Google Gemma 2 2B},
  howpublished={\url{https://huggingface.co/google/gemma-2-2b}},
  year={2024}
}

@misc{GoogleGemma-7b,
  title={Google Gemma 7B},
  howpublished={\url{https://huggingface.co/google/gemma-7b}},
  year={2024}
}

@misc{GoogleGemma-2b,
  title={Google Gemma 2B},
  howpublished={\url{https://huggingface.co/google/gemma-2b}},
  year={2024}
}

@misc{GoogleRecurrentGemma-9b,
  title={Google RecurrentGemma 9B},
  howpublished={\url{https://huggingface.co/google/recurrentgemma-9b}},
  year={2024}
}

@misc{GoogleRecurrentGemma-2b,
  title={Google RecurrentGemma 2B},
  howpublished={\url{https://huggingface.co/google/recurrentgemma-2b}},
  year={2024}
}

% Microsoft Models on Hugging Face
@misc{MicrosoftPhi4-mini-reasoning,
  title={Microsoft Phi 4 Mini Reasoning},
  howpublished={\url{https://huggingface.co/microsoft/Phi-4-mini-reasoning}},
  year={2024}
}

@misc{MicrosoftPhi4-reasoning,
  title={Microsoft Phi 4 Reasoning},
  howpublished={\url{https://huggingface.co/microsoft/Phi-4-reasoning}},
  year={2024}
}

@misc{MicrosoftPhi4-reasoning-plus,
  title={Microsoft Phi 4 Reasoning Plus},
  howpublished={\url{https://huggingface.co/microsoft/Phi-4-reasoning-plus}},
  year={2024}
}

@misc{MicrosoftPhi4,
  title={Microsoft Phi 4},
  howpublished={\url{https://huggingface.co/microsoft/phi-4}},
  year={2024}
}

@misc{MicrosoftPhi2,
  title={Microsoft Phi 2},
  howpublished={\url{https://huggingface.co/microsoft/phi-2}},
  year={2024}
}

@misc{MicrosoftPhi1-5,
  title={Microsoft Phi 1.5},
  howpublished={\url{https://huggingface.co/microsoft/phi-1_5}},
  year={2024}
}

@misc{MicrosoftPhi1,
  title={Microsoft Phi 1},
  howpublished={\url{https://huggingface.co/microsoft/phi-1}},
  year={2024}
}

% OpenAI Models on Hugging Face
@misc{OpenAIGPT-oss-120b,
  title={OpenAI GPT OSS 120B},
  howpublished={\url{https://huggingface.co/openai/gpt-oss-120b}},
  year={2024}
}

@misc{OpenAIGPT-oss-20b,
  title={OpenAI GPT OSS 20B},
  howpublished={\url{https://huggingface.co/openai/gpt-oss-20b}},
  year={2024}
}

@misc{OpenAIGPT2-xl,
  title={OpenAI GPT-2 XL},
  howpublished={\url{https://huggingface.co/openai-community/gpt2-xl}},
  year={2024}
}

@misc{OpenAIGPT2-large,
  title={OpenAI GPT-2 Large},
  howpublished={\url{https://huggingface.co/openai-community/gpt2-large}},
  year={2024}
}

@misc{OpenAIGPT2-medium,
  title={OpenAI GPT-2 Medium},
  howpublished={\url{https://huggingface.co/openai-community/gpt2-medium}},
  year={2024}
}

@misc{OpenAIGPT2,
  title={OpenAI GPT-2},
  howpublished={\url{https://huggingface.co/openai-community/gpt2}},
  year={2024}
}

@misc{MurtaJunior2025,
  title={Relationship between model complexity and inference capability},
  author={Murta, Luiz Otavio},
  note={Personal communication and ongoing research},
  year={2025}
}
