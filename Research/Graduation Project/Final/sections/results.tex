\section{Results}

\subsection{Extraction process}

    The dataset in section \ref{sec:building_testing_dataset} was built successfully through the process described. 

    A model with 10 billion parameters took approximately 3.5 hours to process all weight-type combinations and filtering settings using the workstation described in section \ref{sec:experimental_setup}. The total parameter count of all models is \textbf{652.802.782.352}, taking around \textbf{228 hours} (9.5 days) of computation time in total.

    The resulting dataset contains \textbf{5511 rows}, each representing a unique combination of model, weight-type combination, filtering setting, LMC complexity value, number of histogram bins, and available benchmark results. That was 264 rows short of the expected 5775 rows due to:
    \begin{itemize}
        \item Some unfiltered data models exceeded the maximum number of bins (set to 1 billion) during histogram creation, leading to their exclusion. Increasing the maximum bin count is not possible due to memory constraints and keeping the rows in the dataset capped at 1 billion bins would have introduced bias to the analysis.
        \item Some numerical values such as count, min, max, mean, std, bin\_count, shannon\_entropy, disequilibrium, and complexity were reported as NaN or infinite values in certain model and weight-type combinations, which required their exclusion from the dataset. The reasons are unknown, but the main suspects are floating-point precision errors and divisions by zero in some edge cases.
    \end{itemize}

\subsection{Filter dimension}

    Following the methodology described in section \ref{sec:best_filtering_setting}, we obtained the plots for average and maximum number of histogram bins per filtering setting across all models and weight-type combinations:
    
    \begin{tcolorbox}[colback=gray!10, colframe=black, boxrule=1pt, rounded corners]
        For visualization purposes, unfiltered data is represented as a \textbf{40 $\sigma$} filtering setting in the plots.
    \end{tcolorbox}

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.8\linewidth]{img/filterbins_average_bins_bar.png}
        \caption{Average number of histogram bins per filtering setting.}
        \label{fig:average_bins_per_filtering}
    \end{figure}

    \begin{figure}[H] 
        \centering
        \includegraphics[width=0.8\linewidth]{img/filterbins_maximum_bins_bar.png}
        \caption{Maximum number of histogram bins per filtering setting.}
        \label{fig:maximum_bins_per_filtering}
    \end{figure}

    It is clear that both plots are pretty similar in shape and proportions. There is a big jump downwards from the unfiltered setting (40 $\sigma$) to the first filtered one (20 $\sigma$), then a slow and gradual decrease in the number of bins as the filtering becomes more aggressive.

    We can understand this behavior more clearly by looking at the regression lines:

    \begin{figure}[H] 
        \centering
        \includegraphics[width=0.8\linewidth]{img/filterbins_regression.png}
        \caption{Average and maximum number of histogram bins per filtering setting.}
        \label{fig:regression_bins_per_filtering}
    \end{figure}

    The average bins show a weak fit with $R^2 = 0.045$, indicating high variability. In contrast, the maximum bins demonstrate an exceptional fit with $R^2 = 0.999$. This is expected, as the maximum bins are only one data point per filtering setting, making it easier to fit a curve through them. The fact that both curves follow an \textbf{exponential trend}, however, is noteworthy.

    The maximum bins (green dashed line) explodes faster than the average bins (red line) as the filtering becomes less aggressive, meaning that a worst-case scenario grows more rapidly than the average case. Mathematically, while maximum bins grow at a rate of $e ^{0.250x}$, average bins only grow $e ^{0.236x}$.

    We can also analyze the filtering setting using our second criterion: complexity values. We obtained the following plots for average and maximum LMC complexity values per filtering setting across all models and weight-type combinations:

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.8\linewidth]{img/filtercomplexity_average_complexity_bar.png}
        \caption{Average complexity per filtering setting.}
    \end{figure}

    \begin{figure}[H] 
        \centering
        \includegraphics[width=0.8\linewidth]{img/filtercomplexity_maximum_complexity_bar.png}
        \caption{Maximum complexity per filtering setting.}
    \end{figure}

    We can see that the shapes of the plots are similar to each other but follow a different trend compared to the previous figures \ref{fig:average_bins_per_filtering} and \ref{fig:maximum_bins_per_filtering}. Except for the spike at 0.125 $\sigma$, they appear to follow a \textbf{logarithmic trend}.
    
    Overall, the maximum values tend to be a bit more flat and unstable, while the average values show a smoother curve. The shapes are not as simple as the previous figures \ref{fig:average_bins_per_filtering} and \ref{fig:maximum_bins_per_filtering} since it does not follow a monotonic downward trend. For reasons unknown, the 0.25 $\sigma$ bar is the global minimum as we can observe an upward trend from 0.25 to 0.125 $\sigma$.

    Complexity values in 10 and 20 $\sigma$ are almost identical to the unfiltered setting (40 $\sigma$), followed by a gradual decrease that accelerates as the filtering becomes more aggressive until reaching the global minimum.

    We can take a look at the regression lines to understand this behavior better:

    \begin{figure}[H] 
        \centering
        \includegraphics[width=0.8\linewidth]{img/filtercomplexity_regression.png}
        \caption{Average and maximum number of complexity per filtering setting.}
    \end{figure}

    As expected, both average (red line) and maximum (green dashed line) complexity values follow a \textbf{logarithmic trend}, with $R^2 = 0.014$ and $R^2 = 0.578$ respectively. The maximum fit is much better than the average fit due to the same reasons as explained in figure \ref{fig:regression_bins_per_filtering}. The maximum fit here is, however, much worse than the maximum fit in figure \ref{fig:regression_bins_per_filtering}, probably due to the unusual spike at 0.125 $\sigma$.

    Both curves show a downward trend as the filtering becomes more aggressive that accelerates the more we approach 0 $\sigma$, where all the values are removed. This acceleration is faster in the maximum complexity fit. 
    
    We also observe a very slight downward concavity in both curves approaching the unfiltered setting (40 $\sigma$) since the $x$ terms are slightly negative. For now, we are considering this an artifact of the regression that happens as a consequence of 40 $\sigma$ being an approximation of the unfiltered data even though it is very close.

    Also, it looks like using even less aggressive filtering settings (such as 30 $\sigma$) could be a better choice, but concluding this would require further experiments with more filtering settings between 20 $\sigma$ and the unfiltered data. Further experiments could also be conducted to understand the intriguing spike in complexity at 0.125 $\sigma$ and the behavior between it and totally filtered data (0 $\sigma$). Both of those points are left as future work.

    Analyzing the criteria together, we can conclude that the best filtering setting is \textbf{20 $\sigma$}, as it provides a significant reduction in the number of histogram bins while maintaining an almost identical complexity value compared to the unfiltered data. 
    
\subsection{Weight-type dimension}

    Following the methodology described in section \ref{sec:weight_type_analysis}, we obtained the plots for average and maximum LMC complexity values per weight-type combination across all models using the best filtering setting (20 $\sigma$):

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.85\linewidth]{img/weight_average_complexity_bar.png}
        \caption{Average complexity per weight-type combination.}
    \end{figure}

    \begin{figure}[H] 
        \centering
        \includegraphics[width=0.85\linewidth]{img/weight_maximum_complexity_bar.png}
        \caption{Maximum complexity per weight-type combination.}
    \end{figure}

    Looking at both plots, we can see that the shapes appear to be similar to each other. For a more careful analysis we will divide the ranking in multiple weight types (e.g. bias-embedding; norm-other) and single weight types (e.g. bias; other).

    We could not find a consistent pattern while analyzing the multiple weight types. It is obvious, however, that classes that include \textbf{other} have similar values since \textbf{other} weights are the majority of parameters in a model.

    Analyzing the single weight types, it is possible to see that while \textbf{norm} is responsible for the highest complexity values in any case and \textbf{embedding} is responsible for the lowest in any case, near zero. \textbf{Bias} and \textbf{other} weights vary between second and third place depending on the metric (average or maximum).

    It is unclear why this ranking occurs. As a consequence, it would be interesting to further investigate if training a model while optimizing the complexity value of certain weight types (e.g. norm) could lead to better performance or generalization. However, this is also left as future work.

    We will proceed with the analysis using all weight types combined except \textbf{embedding} (the \textbf{bias-norm-other} combination). This is justified as \textbf{embedding} has the lowest complexity values and is not likely to contribute significantly to the overall complexity of the model. Also because removing them allows us to follow the same methodology as the scaling laws study \cite{Kaplan2020}.

\subsection{Complexity vs Number of parameters}
\label{sec:complexity_vs_num_params_results}

    Following the methodology described in section \ref{sec:complexity_vs_num_params}, we obtained the following plot for average and maximum LMC complexity values against the number of parameters across the bias-norm-other weight-type combination using the best filtering setting (20 $\sigma$):

    \begin{figure}[H] 
        \centering
        \includegraphics[width=1\linewidth]{img/noembed-weights/param_average_complexity_histogram.png}
        \caption{Average complexity vs number of parameters.}
    \end{figure}

    A clear trend emerges from the histogram: as the number of parameters increases, the complexity values tend to stay mostly flat, with a sudden spike at the end (around $10^{10.5}$ to $10^{11.1}$ parameters). 
    
    This indicates that complexity values tend to remain stable with the amount of parameters. The spike at the end is, however, unexpected and its causes are unknown. Further investigation is required to understand if this is an artifact of the dataset or a genuine phenomenon.

    Since number of parameters is a known predictor of model performance \cite{Kaplan2020}, we can conclude right away that the high average complexity values found in the spike region, genuine or not, are likely to induce a bias towards better performance in the next analysis.

\subsection{Complexity vs Number of bins}

    Following the methodology described in section \ref{sec:complexity_vs_num_bins}, we obtained the following plot for average LMC complexity values against the number of histogram bins across the bias-norm-other weight-type combination using the best filtering setting (20 $\sigma$):

    \begin{figure}[H] 
        \centering
        \includegraphics[width=1\linewidth]{img/noembed-weights/bin_average_complexity_histogram.png}
        \caption{Average complexity vs number of histogram bins.}   
    \end{figure}

    Again, a clear trend emerges, almost identical to the previous section (\ref{sec:complexity_vs_num_params_results}). That indicates that there may be a strong correlation between the number of parameters and the number of histogram bins. A possible explanation is that larger models tend to have a wider range of weight values, leading to more histogram bins when using the Freedman-Diaconis choice \cite{FreedmanDiaconis1981}.

\subsection{Complexity vs Benchmarks}
\label{sec:complexity_vs_benchmark_performance_results}

    Following the methodology described in section \ref{sec:complexity_vs_benchmark_performance}, using the bias-norm-other combination and the best filtering setting (20 $\sigma$) dataset, we made a control plot using a known correlation: parameter count vs benchmark performance. The results are as follows:

    \begin{figure}[H] 
        \centering
        \includegraphics[width=0.8\linewidth]{img/noembed-weights/control_correlation_comparison.png}
        \caption{Pearson correlation for parameter count vs benchmark performance.}
        \label{fig:control_correlation_comparison}
    \end{figure}

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.8\linewidth]{img/noembed-weights/control_r2_linear_comparison.png}
        \caption{$R^2$ values for parameter count vs benchmark performance.}
        \label{fig:control_r2_linear_comparison}
    \end{figure}

    As expected, all the benchmarks show a positive correlation with parameter count, validating the control set. It is also noticeable that the linear regression $R^2$ values are relatively low for the concatenation of all benchmarks (\textbf{all}), indicating a non-linear relationship between parameter count and benchmark performance.

    Now, we can analyze the actual relationship of interest: LMC complexity vs benchmark performance. The results are as follows:

    \begin{figure}[H] 
        \centering
        \includegraphics[width=0.8\linewidth]{img/noembed-weights/complexity_correlation_comparison.png}
        \caption{Pearson correlation for Complexity vs benchmark performance.}   
        \label{fig:complexity_correlation_comparison}
    \end{figure}

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.8\linewidth]{img/noembed-weights/complexity_r2_linear_comparison.png}
        \caption{$R^2$ values for complexity vs benchmark performance.}
        \label{fig:complexity_r2_linear_comparison}
    \end{figure}

    It is immediately clear that results are inconsistent across benchmarks. While \textbf{LMArena} and \textbf{MMLU} show a positive correlation between complexity and performance, \textbf{MMLU-Pro} and \textbf{OpenLLM} show a negative correlation.

    For individual benchmarks, the negative correlations have a smaller absolute value compared to the positive correlations, indicating a bias towards positive correlation. The correlation values are lower than the control's in all cases. The $R^2$ values are also lower than the control's in all cases, which might indicate an even less linear relationship between complexity and benchmark performance compared to control. It can also indicate that complexity is not a good predictor of benchmark performance.
    
    However, the \textbf{all} correlation, formed by analyzing a regression aggregating all data points from all benchmarks, shows an impressive positive correlation of 0.1625 which is higher than the control's 0.1488. The $R^2$ scores are still low but also slightly higher than the control's (0.0264 vs 0.0221).

    This is an interesting result: while individual benchmarks show inconsistent correlations and score consistently worse than control, the aggregation of all benchmarks is better than control. This suggests that there may be a general trend of increasing benchmark performance with increasing LMC complexity when considering a diverse set of benchmarks. Since benchmarks are an imperfect measure of model performance, it is possible that individual benchmark inconsistencies are smoothed out.

    We also found the statistical significance of the correlations found using t-tests \cite{Fisher1921}:
    
    \begin{table}[H]
        \centering
        \begin{tabular}{|c|c|c|c|c|c|}
            \hline
            \textbf{Benchmark} & \textbf{r} & \textbf{n} & \textbf{t} & \textbf{p-value} & \textbf{Significance} \\
            \hline
            LMArena & 0.3860 & 21 & 1.8239 & 0.0839 & No \\
            \hline
            MMLU & 0.4176 & 26 & 2.2515 & 0.0338 & Yes \\
            \hline
            MMLU-Pro & -0.1813 & 15 & -0.6647 & 0.5179 & No \\
            \hline
            OpenLLM & -0.2534 & 24 & -1.2287 & 0.2322 & No \\
            \hline
            all & 0.3066 & 86 & 2.9522 & 0.004089 & Yes \\
            \hline
        \end{tabular}
        \caption{Statistical significance of Pearson correlations between complexity and benchmark performance.}
        \label{tab:statistical_significance}
    \end{table}

    It is possible to observe that only the MMLU correlation is statistically significant at a 0.05 significance level. The most important correlation, however, the \textbf{all} correlation, is not. 

    The margins of error are quite high due to the small sample sizes (n < 30), which makes it difficult to draw strong conclusions from the correlations found. Even with small sample sizes, however, we can see that the correlations found are not that weak: \textbf{LMArena} and \textbf{all} were close to significance despite having only 21 and 86 data points respectively.

    Interestingly, the benchmarks that are the furthest from significance are the ones with negative correlation (\textbf{MMLU-Pro} and \textbf{OpenLLM}), further indicating that there is more likely a positive bias towards correlation between complexity and benchmark performance than the other way around.

    To better understand the shape of the relationship, we can look at the free and linear regression lines for the complexity plots:

    \begin{figure}[H] 
        \centering
        \includegraphics[width=0.65\linewidth]{img/noembed-weights/comp_lmarena_regression.png}
        \caption{LMC complexity vs LMArena benchmark.}
        \label{fig:comp_lmarena_regression}
    \end{figure}

    Here it is possible to see that the data points follow an almost perfect constant trend with two outliers that drive the positive correlation. This is likely related to the result seen in section \ref{sec:complexity_vs_num_params_results}, where complexity values remain mostly flat with a sudden spike at the end.

    \begin{figure}[H] 
        \centering
        \includegraphics[width=0.65\linewidth]{img/noembed-weights/comp_mmlu_regression.png}
        \caption{LMC complexity vs MMLU benchmark.}
    \end{figure}

    Again, a nearly constant trend is observed with a sudden spike at the end driving the positive correlation. Here we can see that the free regression fits an \textbf{exponential}, which can be the explanation for the apparently constant trend in both this and figure \ref{fig:comp_lmarena_regression} since exponentials can appear almost constant for small x values.

    \begin{figure}[H] 
        \centering
        \includegraphics[width=0.65\linewidth]{img/noembed-weights/comp_mmlupro_regression.png}
        \caption{LMC complexity vs MMLU-Pro benchmark.}
    \end{figure}

    Here we see a different behavior: a slightly downward linear trend with high variability and two outliers in the middle driving the negative correlation.

    \begin{figure}[H] 
        \centering
        \includegraphics[width=0.65\linewidth]{img/noembed-weights/comp_openllm_regression.png}
        \caption{LMC complexity vs OpenLLM benchmark.}
    \end{figure}

    And here the most interesting behavior is observed: two lines are formed, one with a slight logarithmic upward trend and another with a slight logarithmic downward trend, both regression fits trying to find a middle ground between them.

    \begin{figure}[H] 
        \centering
        \includegraphics[width=0.65\linewidth]{img/noembed-weights/comp_all_regression.png}
        \caption{LMC complexity vs all benchmarks aggregated.}
    \end{figure}

    In all cases, we can see that the free regressions do not appear to be able to capture the behavior of the datapoints well due to their outliers. The linear regressions are also influenced by the same factor, leading to low $R^2$ values. This is a further indicator that LMC complexity appears not to be a reliable predictor of benchmark performance.

    As a bonus, for the sake of completeness, we made a bar plot ranking the top 20 configurations (model + weight-type combination) sorted by highest correlation between LMC complexity and benchmark performance (all benchmarks aggregated):

    \begin{figure}[H] 
        \centering
        \includegraphics[width=0.8\linewidth]{img/all-weights/top_20_correlations.png}
        \caption{Top 20 configurations by Pearson correlation.}
    \end{figure}
