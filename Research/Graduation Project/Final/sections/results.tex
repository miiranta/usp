\section{Results}

\subsection{Extraction process}

    The dataset in section \ref{sec:building_testing_dataset} was built successfully through the process described. 

    A model with 10 billion parameters took approximately 3.5 hours to process all weight-type combinations and filtering settings using the workstation described in section \ref{sec:experimental_setup}. The total parameter count of all models is \textbf{652.802.782.352}, taking around \textbf{228 hours} (9.5 days) of computation time in total.

    The resulting dataset contains \textbf{5511 rows}, each representing a unique combination of model, weight-type combination, filtering setting, LMC complexity value, number of histogram bins, and available benchmark results. That was 264 rows short of the expected 5775 rows due to:
    \begin{itemize}
        \item Some unfiltered data models exceeded the maximum number of bins (set to 1 billion) during histogram creation, leading to their exclusion. Increasing the maximum bin count is not possible due to memory constraints and keeping the rows in the dataset capped at 1 billion bins would have introduced bias to the analysis.
        \item Some numerical values such as count, min, max, mean, std, bin\_count, shannon\_entropy, disequilibrium, and complexity were reported as NaN or infinite values in certain model and weight-type combinations, which required their exclusion from the dataset. The reasons are unknown, but the main suspects are floating-point precision errors and divisions by zero in some edge cases.
    \end{itemize}

\subsection{Filter dimension}

    Following the methodology described in section \ref{sec:best_filtering_setting}, we obtained the plots for average and maximum number of histogram bins per filtering setting across all models and weight-type combinations:
    
    \begin{tcolorbox}[colback=gray!10, colframe=black, boxrule=1pt, rounded corners]
        For visualization purposes, unfiltered data is represented as a \textbf{40 $\sigma$} filtering setting in the plots.
    \end{tcolorbox}

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.8\linewidth]{img/filterbins_average_bins_bar.png}
        \caption{Average number of histogram bins per filtering setting.}
        \label{fig:average_bins_per_filtering}
    \end{figure}

    \begin{figure}[H] 
        \centering
        \includegraphics[width=0.8\linewidth]{img/filterbins_maximum_bins_bar.png}
        \caption{Maximum number of histogram bins per filtering setting.}
        \label{fig:maximum_bins_per_filtering}
    \end{figure}

    It is clear that both plots are pretty similar in shape and proportions. There is a big jump downwards from the unfiltered setting (40 $\sigma$) to the first filtered one (20 $\sigma$), then a slow and gradual decrease in the number of bins as the filtering becomes more aggressive.

    We can understand this behavior more clearly by looking at the regression lines:

    \begin{figure}[H] 
        \centering
        \includegraphics[width=0.8\linewidth]{img/filterbins_regression.png}
        \caption{Free regression lines for average and maximum number of histogram bins per filtering setting.}
        \label{fig:regression_bins_per_filtering}
    \end{figure}

    The average bins show a weak fit with $R^2 = 0.044$, indicating high variability. In contrast, the maximum bins demonstrate an exceptional fit with $R^2 = 0.999$. This is expected, as the maximum bins are only one data point per filtering setting, making it easier to fit a curve through them. The fact that both curves follow an \textbf{exponential trend}, however, is noteworthy.

    The maximum bins (green dashed line) explodes much faster than the average bins (red line) as the filtering becomes less aggressive, meaning that a worst case scenario grows much more rapidly than the average case.

    We can also analyze the filtering setting using our second criterion: complexity values. We obtained the following plots for average and maximum LMC complexity values per filtering setting across all models and weight-type combinations:

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.8\linewidth]{img/filtercomplexity_average_complexity_bar.png}
        \caption{Average complexity per filtering setting.}
    \end{figure}

    \begin{figure}[H] 
        \centering
        \includegraphics[width=0.8\linewidth]{img/filtercomplexity_maximum_complexity_bar.png}
        \caption{Maximum complexity per filtering setting.}
    \end{figure}

    We can see that the shape of the plots is similar to each other but follows a different trend compared to the previous figures \ref{fig:average_bins_per_filtering} and \ref{fig:maximum_bins_per_filtering}, more similar to a \textbf{logarithmic trend}.
    
    Overall, the maximum values tend to be a bit more flat and unstable, while the average values show a smoother curve. The shapes are not as simple as the previous figures \ref{fig:average_bins_per_filtering} and \ref{fig:maximum_bins_per_filtering}, since it does not follow a monotonic downward trend, we can observe an upward trend from 0.25 to 0.125 $\sigma$. For reasons unknown, the 0.25 $\sigma$ bar is a global minimum while we have a spike at 0.125 $\sigma$.

    Complexity values in 10 and 20 $\sigma$ are almost identical to the unfiltered setting (40 $\sigma$), followed by a gradual decrease as the filtering becomes more aggressive until reaching the global minimum at 0.25 $\sigma$.

    We can take a look at the regression lines to understand this behavior better:

    \begin{figure}[H] 
        \centering
        \includegraphics[width=0.8\linewidth]{img/filtercomplexity_regression.png}
        \caption{Free regression lines for average and maximum number of complexity per filtering setting.}
    \end{figure}

    As expected, both average (red line) and maximum (green dashed line) complexity values follow a \textbf{logarithmic trend}, with $R^2 = 0.014$ and $R^2 = 0.659$ respectively. The maximum fit is much better than the average fit due to the same reasons as explained in figure \ref{fig:regression_bins_per_filtering}, it is, however, much worse than the maximum fit in figure \ref{fig:regression_bins_per_filtering}, probably due to the unusual spike at 0.125 $\sigma$.

    Both curves show a downward trend as the filtering becomes more aggressive that accelerates the more we approach 0 $\sigma$, where all the values are removed. This acceleration is faster in the maximum complexity fit.

    Analyzing both criteria together, we can conclude that the best filtering setting is \textbf{20 $\sigma$}, as it provides a significant reduction in the number of histogram bins while maintaining an almost identical complexity value compared to the unfiltered data. 
    
    It looks like using even less aggressive filtering settings (such as 30 $\sigma$) could be a better choice, but concluding this would require further experiments with more filtering settings between 20 $\sigma$ and the unfiltered data. More experiments could also be done to understand the intriguing spike in complexity at 0.125 $\sigma$ and the behavior between it and totally filtered data (0 $\sigma$). However, both of those points are left as future work.

\subsection{Weight-type dimension}

    Following the methodology described in section \ref{sec:weight_type_analysis}, we obtained the plots for average and maximum LMC complexity values per weight-type combination across all models using the best filtering setting (20 $\sigma$):

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.8\linewidth]{img/weight_average_complexity_bar.png}
        \caption{Average complexity per weight-type combination.}
    \end{figure}

    \begin{figure}[H] 
        \centering
        \includegraphics[width=0.8\linewidth]{img/weight_maximum_complexity_bar.png}
        \caption{Maximum complexity per weight-type combination.}
    \end{figure}

    Again, looking at both plots, we can see that the shapes are similar to each other. In this case we have some variation between the rankings of multiple weight classes (e.g. bias-embedding), which is expected since maximum values can be dominated by outliers. The ranking for single weight types (e.g. bias) is the same across both plots.

    Analyzing the multiple weight classes, we can't find a consistent pattern. It is obvious that classes with \textbf{other} have similar values since \textbf{other} weights are the majority of parameters in a model. It is also interesting to see that \textbf{norm} and \textbf{bias} individually have higher average complexity values than when combined with each other.

    Analyzing the single weight types it is possible to observe that \textbf{norm} weights have the highest complexity values, followed by \textbf{bias} weights, then \textbf{other}, then \textbf{embedding}. While norm is responsible for the highest complexity values, embeddings are close to zero.

    It is unclear why this ranking occurs. It appears to indicate that removing embeddings from a model, like it was done in the scaling laws study \cite{Kaplan2020} won't significantly change its complexity value. 

    It would be interesting to further investigate if training a model while optimizing the complexity value of certain weight types (e.g. norm) could lead to better performance or generalization. However, this is also left as future work.

    We will proceed with the analysis using all weight types combined except embeddings (the \textbf{bias-norm-other} combination). This is justified as embeddings have the lowest complexity values and are not likely to contribute significantly to the overall complexity of the model. Also because removing them allows us to follow the same methodology as the scaling laws study \cite{Kaplan2020}.

\subsection{Complexity vs Number of parameters}
\label{sec:complexity_vs_num_params_results}

    Following the methodology described in section \ref{sec:complexity_vs_num_params}, we obtained the following plot for average and maximum LMC complexity values against the number of parameters across the bias-norm-other weight-type combination using the best filtering setting (20 $\sigma$):

    \begin{figure}[H] 
        \centering
        \includegraphics[width=1\linewidth]{img/noembed-weights/param_average_complexity_histogram.png}
        \caption{Average complexity vs number of parameters.}
    \end{figure}

    A clear trend emerges from the histogram: as the number of parameters increases, the average complexity tends to decrease, with a sudden spike at the end (around $10^{10.9}$ parameters). This indicates that larger models generally exhibit lower complexity values. The spike at the end is, however, unexpected and its causes are unknown.

\subsection{Complexity vs Number of bins}

    Following the methodology described in section \ref{sec:complexity_vs_num_bins}, we obtained the following plot for average LMC complexity values against the number of histogram bins across the bias-norm-other weight-type combination using the best filtering setting (20 $\sigma$):

    \begin{figure}[H] 
        \centering
        \includegraphics[width=1\linewidth]{img/noembed-weights/bin_average_complexity_histogram.png}
        \caption{Average complexity vs number of histogram bins.}   
    \end{figure}

    Again, a clear trend emerges, almost identical to the previous section (\ref{sec:complexity_vs_num_params_results}). That indicates that there may be a strong correlation between the number of parameters and the number of histogram bins. A possible explanation is that larger models tend to have a wider range of weight values, leading to more histogram bins when using the Freedman-Diaconis choice \cite{FreedmanDiaconis1981}.

\subsection{Complexity vs Benchmarks}

    Following the methodology described in section \ref{sec:complexity_vs_benchmark_performance}, using the bias-norm-other combination and the best filtering setting (20 $\sigma$) dataset, we made a control plot using a known correlation: parameter count vs benchmark performance. The results are as follows:

    \begin{figure}[H] 
        \centering
        \includegraphics[width=0.8\linewidth]{img/noembed-weights/control_correlation_comparison.png}
        \caption{Pearson correlation for parameter count vs benchmark performance.}
        \label{fig:control_correlation_comparison}
    \end{figure}

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.8\linewidth]{img/noembed-weights/control_r2_linear_comparison.png}
        \caption{Linear regression $R^2$ values for parameter count vs benchmark performance.}
        \label{fig:control_r2_linear_comparison}
    \end{figure}

    As expected, all the benchmarks show a positive correlation with parameter count. The linear regression $R^2$ values are lower, indicating that the relationship is not strictly linear.

    Now, we can analyze the actual relationship of interest: LMC complexity vs benchmark performance. The results are as follows:

    \begin{figure}[H] 
        \centering
        \includegraphics[width=0.8\linewidth]{img/noembed-weights/complexity_correlation_comparison.png}
        \caption{Pearson correlation for Complexity vs benchmark performance.}   
        \label{fig:complexity_correlation_comparison}
    \end{figure}

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.8\linewidth]{img/noembed-weights/complexity_r2_linear_comparison.png}
        \caption{Linear regression $R^2$ values for complexity vs benchmark performance.}
        \label{fig:complexity_r2_linear_comparison}
    \end{figure}

    It is immediately clear that results are inconsistent across benchmarks. While \textbf{LMArena} and \textbf{MMLU} show a positive correlation between complexity and performance, \textbf{MMLU-Pro} and \textbf{OpenLLM} show a negative correlation.

    The negative correlations have a smaller absolute value compared to the positive correlations, indicating a bias towards positive correlation. The \textbf{all} correlation, formed by analyzing a regression aggregating all data points from all benchmarks, shows a small positive correlation of 0.02, confirming this bias.

    The \textbf{all} correlation between control and complexity plots is about 5 times smaller (0.10 vs 0.02), indicating that complexity is a much weaker predictor of benchmark performance compared to parameter count. $R^2$ values are also much smaller in the complexity plot, further confirming the non-linearity of the relationship.

    We also found the statistical significance of the correlations found using t-tests \cite{Fisher1921}:
    
    \begin{table}[H]
        \centering
        \begin{tabular}{|c|c|c|c|c|c|}
            \hline
            \textbf{Benchmark} & \textbf{r} & \textbf{n} & \textbf{t} & \textbf{p-value} & \textbf{Significance} \\
            \hline
            LMArena & 0.3860 & 21 & 1.8239 & 0.0839 & No \\
            \hline
            MMLU & 0.4176 & 26 & 2.2515 & 0.0338 & Yes \\
            \hline
            MMLU-Pro & -0.1813 & 15 & -0.6647 & 0.5179 & No \\
            \hline
            OpenLLM & -0.2534 & 24 & -1.2287 & 0.2322 & No \\
            \hline
            all & 0.1625 & 86 & 1.5094 & 0.1349 & No \\
            \hline
        \end{tabular}
        \caption{Statistical significance of Pearson correlations between complexity and benchmark performance.}
        \label{tab:statistical_significance}
    \end{table}

    Interestingly, only LMArena, MMLU and MMLU-Pro correlations are statistically significant at a 0.05 significance level. The most important correlation, however, the \textbf{all} correlation, is not statistically significant by a wide margin, meaning more data would be required to confirm or deny the existence of a general correlation between complexity and benchmark performance.

    To better understand the shape of the relationship, we can look at the free and linear regression lines for the complexity plots:

    \begin{figure}[H] 
        \centering
        \includegraphics[width=0.72\linewidth]{img/noembed-weights/comp_lmarena_regression.png}
        \caption{Regression lines for LMC complexity vs LMArena benchmark.}
    \end{figure}

    \begin{figure}[H] 
        \centering
        \includegraphics[width=0.72\linewidth]{img/noembed-weights/comp_mmlu_regression.png}
        \caption{Regression lines for LMC complexity vs MMLU benchmark.}
    \end{figure}

    \begin{figure}[H] 
        \centering
        \includegraphics[width=0.72\linewidth]{img/noembed-weights/comp_mmlupro_regression.png}
        \caption{Regression lines for LMC complexity vs MMLU-Pro benchmark.}
    \end{figure}

    \begin{figure}[H] 
        \centering
        \includegraphics[width=0.72\linewidth]{img/noembed-weights/comp_openllm_regression.png}
        \caption{Regression lines for LMC complexity vs OpenLLM benchmark.}
    \end{figure}

    We can see that the free regressions can't agree on a common trend since there are \textbf{logarithmic}, \textbf{exponential} and \textbf{quadratic} best fits across the different benchmarks. The benchmarks with negative correlation (MMLU-Pro and OpenLLM) at least agree on having a \textbf{logarithmic} trend, but the first one is increasing (+ln(x)) while the second one is decreasing (-ln(x)).

    This further confirms that LMC complexity is not a reliable predictor of benchmark performance, as different benchmarks show conflicting relationships with complexity values.

    As a bonus, for the sake of completeness, we made a bar plot ranking the top 20 configurations (model + weight-type combination) sorted by highest correlation between LMC complexity and benchmark performance (all benchmarks aggregated):

    \begin{figure}[H] 
        \centering
        \includegraphics[width=0.8\linewidth]{img/all-weights/top_20_correlations.png}
        \caption{Top 20 configurations by Pearson correlation.}
    \end{figure}
