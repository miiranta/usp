\section{Results}

\subsection{Extraction process}

    The dataset in section \ref{sec:building_testing_dataset} was built successfully through the process described. 

    A model with 10 billion parameters took approximately 3.5 hours to process all weight-type combinations and filtering settings using the workstation described in section \ref{sec:experimental_setup}. The total parameter count of all models is \textbf{652.802.782.352}, taking around \textbf{228 hours} (9.5 days) of computation time in total.

    The resulting dataset contains \textbf{5676 rows}, each representing a unique combination of model, weight-type combination, filtering setting, LMC complexity value, number of histogram bins, and available benchmark results. That was 99 rows short of the expected 5775 rows due to:
    \begin{itemize}
        \item Some unfiltered data models exceeded the maximum number of bins (set to 1 billion) during histogram creation, leading to their exclusion. Increasing the maximum bin count is not possible due to memory contraints and keeping the rows in the dataset capped at 1 billion bins would have introduced bias to the analysis.
        \item Some numerical values such as count, min, max, mean, std, bin\_count, shannon\_entropy, desequilibrium, and complexity were reported as NaN or infinite values in certain model and weight-type combinations, which required their exclusion from the dataset. The reasons are unknown, but the main suspects are floating-point precision errors and divisions by zero in some edge cases.
    \end{itemize}

\subsection{Filter dimension}

    Following the methodology described in section \ref{sec:best_filtering_setting}, we obtained the plots for average and maximum number of histogram bins per filtering setting across all models and weight-type combinations:
    
    \begin{tcolorbox}[colback=gray!10, colframe=black, boxrule=1pt, rounded corners]
        For visualization purposes, unfiltered data is represented as a \textbf{40 $\sigma$} filtering setting in the plots.
    \end{tcolorbox}

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.8\linewidth]{img/filterbins_average_bins_bar.png}
        \caption{Average number of histogram bins per filtering setting.}
        \label{fig:average_bins_per_filtering}
    \end{figure}

    \begin{figure}[H] 
        \centering
        \includegraphics[width=0.8\linewidth]{img/filterbins_maximum_bins_bar.png}
        \caption{Maximum number of histogram bins per filtering setting.}
        \label{fig:maximum_bins_per_filtering}
    \end{figure}

    It is clear that both plots are pretty similar in shape and proportions. There is a big jump downwards from the unfiltered setting (40 $\sigma$) to the first filtered one (20 $\sigma$), then a slow and gradual decrease in the number of bins as the filtering becomes more aggressive.

    We can understand this behavior more clearly by looking at the regression lines:

    \begin{figure}[H] 
        \centering
        \includegraphics[width=0.8\linewidth]{img/filterbins_regression.png}
        \caption{Free regression lines for average and maximum number of histogram bins per filtering setting.}
        \label{fig:regression_bins_per_filtering}
    \end{figure}

    The average bins show a weak fit with $R^2 = 0.044$, indicating high variability. In contrast, the maximum bins demonstrate an exceptional fit with $R^2 = 0.999$. This is expected, as the maximum bins are only one data point per filtering setting, making it easier to fit a curve through them. The fact that both curves follow an \textbf{exponential trend}, however, is noteworthy.

    The maximum bins (green dashed line) explodes much faster than the average bins (red line) as the filtering becomes less aggressive, meaning that a worst case scenario grows much more rapidly than the average case.

    We can also analyze the filtering setting using our second criterion: complexity values. We obtained the following plots for average and maximum LMC complexity values per filtering setting across all models and weight-type combinations:

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.8\linewidth]{img/filtercomplexity_average_complexity_bar.png}
        \caption{Average number of complexity per filtering setting.}
    \end{figure}

    \begin{figure}[H] 
        \centering
        \includegraphics[width=0.8\linewidth]{img/filtercomplexity_maximum_complexity_bar.png}
        \caption{Maximum number of complexity per filtering setting.}
    \end{figure}

    We can see that the shape of the plots are similar to each other but follow a different trend compared to the previous figures \ref{fig:average_bins_per_filtering} and \ref{fig:maximum_bins_per_filtering}, more similar to a \textbf{logarithmic trend}.
    
    Overall, the maximum values tend to be a bit more flat and unstable, while the average values show a smoother curve. The shapes are not as simple as the previous figures \ref{fig:average_bins_per_filtering} and \ref{fig:maximum_bins_per_filtering}, since it does not follow a monotonous downward trend, we can observe an upward trend from 0.25 to 0.125 $\sigma$. For reasons unknown, the 0.25 $\sigma$ bar is a global minimum while we have a spike at 0.125 $\sigma$.

    Complexity values in 10 and 20 $\sigma$ are almost identical to the unfiltered setting (40 $\sigma$), followed by a gradual decrease as the filtering becomes more aggressive until reaching the global minimum at 0.25 $\sigma$.

    We can take a look at the regression lines to understand this behavior better:

    \begin{figure}[H] 
        \centering
        \includegraphics[width=0.8\linewidth]{img/filtercomplexity_regression.png}
        \caption{Free regression lines for average and maximum number of complexity per filtering setting.}
    \end{figure}

    As expected, both average (red line) and maximum (green dashed line) complexity values follow a \textbf{logarithmic trend}, with $R^2 = 0.014$ and $R^2 = 0.659$ respectively. The maximum fit is much better than the average fit due to the same reasons as explained in figure \ref{fig:regression_bins_per_filtering}, it is, however, much worse than the maximum fit in figure \ref{fig:regression_bins_per_filtering}, probably due to the unusual spike at 0.125 $\sigma$.

    Both curves show a downward trend as the filtering becomes more aggressive that accelerates the more we approach 0 $\sigma$, where all the values are removed. This acceleration is faster in the maximum complexity fit.

    Analyzing both criteria together, we can conclude that the best filtering setting is \textbf{20 $\sigma$}, as it provides a significant reduction in the number of histogram bins while maintaining an almost identical complexity value compared to the unfiltered data. 
    
    It looks like that using even less agressive filtering settings (such as 30 $\sigma$) could be a better choice, but concluding this would require further experiments with more filtering settings between 20 $\sigma$ and the unfiltered data. More experiments could also be done to understand the intriguing spike in complexity at 0.125 $\sigma$ and how is the behaviour between it and totally filtered data (0 $\sigma$). However, both of those points are left as future work.

\subsection{Weight-type dimension}

    Following the methodology described in section \ref{sec:weight_type_analysis}, we obtained the plots for average and maximum LMC complexity values per weight-type combination across all models using the best filtering setting (20 $\sigma$):

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.8\linewidth]{img/weight_average_complexity_bar.png}
        \caption{Average complexity per weight-type combination.}
    \end{figure}

    \begin{figure}[H] 
        \centering
        \includegraphics[width=0.8\linewidth]{img/weight_maximum_complexity_bar.png}
        \caption{Maximum complexity per weight-type combination.}
    \end{figure}

    Again, looking at both plots, we can see that the shapes are similar to each other. In this case we have some variation between the rankings of multiple weight classes (e.g. bias-embedding), which is expected since maximum values can be dominated by outliers. The ranking for single weight types (e.g. bias) is the same across both plots.

    Analyzing the multiple weight classes, we can't find a consistent pattern. It is obvious that classes with \textbf{other} have similar values since \textbf{other} weights are the majority of parameters in a model. It is also interesting to see that \textbf{norm} and \textbf{bias} individually have higher average complexity values than when combined with each other.

    Analyzing the single weight types it is possible to observe that \textbf{norm} weights have the highest complexity values, followed by \textbf{bias} weights, then \textbf{other}, then \textbf{embedding}. While norm is responsible for the highest complexity values, embedding are close to zero.

    It is unclear why this ranking occurs. It appears to indicate that removing embeddings from a model, like it was done in the scaling laws study \cite{Kaplan2020} won't significantly change its complexity value. 

    It would be interesting to further investigate if training a model while optimizing the complexity value of certain weight types (e.g. norm) could lead to better performance or generalization. However, this is also left as future work.

\subsection{Complexity vs Number of parameters}
\label{sec:complexity_vs_num_params_results}

    Following the methodology described in section \ref{sec:complexity_vs_num_params}, we obtained the following plot for average and maximum LMC complexity values against the number of parameters across all weight-type combinations using the best filtering setting (20 $\sigma$):

    \begin{figure}[H] 
        \centering
        \includegraphics[width=1\linewidth]{img/param_average_complexity_histogram.png}
        \caption{Average complexity vs number of parameters.}
    \end{figure}

    A clear trend emerges from the histogram: as the number of parameters increases, the average complexity tends to decrease, with a sudden spike at the end (around $10^{10.9}$ parameters). This indicates that larger models generally exhibit lower complexity values. The spike at the end is, however, unexpected and its causes are unknown.

\subsection{Complexity vs Number of bins}

    Following the methodology described in section \ref{sec:complexity_vs_num_bins}, we obtained the following plot for average LMC complexity values against the number of histogram bins across all weight-type combinations using the best filtering setting (20 $\sigma$):

    \begin{figure}[H] 
        \centering
        \includegraphics[width=1\linewidth]{img/bin_average_complexity_histogram.png}
        \caption{Average complexity vs number of histogram bins.}   
    \end{figure}

    Again, a clear trend emerges, almost identical to the previous section (\ref{sec:complexity_vs_num_params_results}). That indicates that there may be a strong correlation between the number of parameters and the number of histogram bins. A possible explanation is that larger models tend to have a wider range of weight values, leading to more histogram bins when using the Freedman-Diaconis choice \cite{FreedmanDiaconis1981}.

\subsection{Complexity vs Benchmarks}

   










