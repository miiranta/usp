\chapter{Results}

\section{Extraction process}

    The dataset in section \ref{sec:building_testing_dataset} was built successfully through the process described. 

    A model with 10 billion parameters took approximately 3.5 hours to process all weight-type combinations and filtering settings using the workstation described in section \ref{sec:experimental_setup}. The total parameter count of all models is \textbf{652.802.782.352}, taking around \textbf{228 hours} (9.5 days) of computation time in total.

    The resulting dataset contains \textbf{5511 rows}, each representing a unique combination of model, weight-type combination, filtering setting, LMC complexity value, number of histogram bins, and available benchmark results. That was 264 rows short of the expected 5775 rows due to:
    \begin{itemize}
        \item Some unfiltered data models exceeded the maximum number of bins (set to 1 billion) during histogram creation, leading to their exclusion. Increasing the maximum bin count is not possible due to memory constraints and keeping the rows in the dataset capped at 1 billion bins would have introduced bias to the analysis.
        \item Some numerical values such as count, min, max, mean, std, bin\_count, shannon\_entropy, disequilibrium, and complexity were reported as NaN or infinite values in certain model and weight-type combinations, which required their exclusion from the dataset. The reasons are unknown, but the main suspects are floating-point precision errors and divisions by zero in some edge cases.
    \end{itemize}

\section{Filter dimension}

    Following the methodology described in section \ref{sec:best_filtering_setting}, we obtained the plots for average and maximum number of histogram bins per filtering setting across all models and weight-type combinations:
    
    \begin{tcolorbox}[colback=gray!10, colframe=black, boxrule=1pt, rounded corners]
        For visualization purposes, unfiltered data is represented as a \textbf{40 $\sigma$} filtering setting in the plots.
    \end{tcolorbox}

    \begin{figure}[H]
        \centering
        \includegraphics[width=1.0\linewidth, height=0.35\textheight, keepaspectratio]{img/filterbins_average_bins_bar.png}
        \caption{Average number of histogram bins per filtering setting.}
        \label{fig:average_bins_per_filtering}
    \end{figure}

    \begin{figure}[H] 
        \centering
        \includegraphics[width=1.0\linewidth, height=0.35\textheight, keepaspectratio]{img/filterbins_maximum_bins_bar.png}
        \caption{Maximum number of histogram bins per filtering setting.}
        \label{fig:maximum_bins_per_filtering}
    \end{figure}

    It is clear that both plots are pretty similar in shape and proportions. There is a big jump downwards from the unfiltered setting (40 $\sigma$) to the first filtered one (20 $\sigma$), then a slow and gradual decrease in the number of bins as the filtering becomes more aggressive.

    We can understand this behavior more clearly by looking at the regression lines:

    \begin{figure}[H] 
        \centering
        \includegraphics[width=1.0\linewidth, height=0.4\textheight, keepaspectratio]{img/filterbins_regression.png}
        \caption{Average and maximum number of histogram bins per filtering setting.}
        \label{fig:regression_bins_per_filtering}
    \end{figure}

    The average bins show a weak fit ($R^2 = 0.045$), indicating high variability. In contrast, the maximum bins demonstrate an exceptional fit ($R^2 = 0.999$). This is expected, as the maximum bins are only one data point per filtering setting, making it easier to fit a curve through them. The fact that both curves follow an \textbf{exponential trend}, however, is noteworthy.

    The maximum bins (green dashed line) explodes faster than the average bins (red line) as the filtering becomes less aggressive, meaning that a worst-case scenario grows more rapidly than the average case. Mathematically, while maximum bins grow at a rate of $e ^{0.250x}$, average bins only grow $e ^{0.236x}$.

    We can also analyze the filtering setting using our second criterion: complexity values. We obtained the following plots for average and maximum LMC complexity values per filtering setting across all models and weight-type combinations:

    \begin{figure}[H]
        \centering
        \includegraphics[width=1.0\linewidth, height=0.35\textheight, keepaspectratio]{img/filtercomplexity_average_complexity_bar.png}
        \caption{Average complexity per filtering setting.}
    \end{figure}

    \begin{figure}[H] 
        \centering
        \includegraphics[width=1.0\linewidth, height=0.35\textheight, keepaspectratio]{img/filtercomplexity_maximum_complexity_bar.png}
        \caption{Maximum complexity per filtering setting.}
    \end{figure}

    We can see that the shapes of the plots are similar to each other but follow a different trend compared to the previous figures \ref{fig:average_bins_per_filtering} and \ref{fig:maximum_bins_per_filtering}. Except for the spike at 0.125 $\sigma$, they appear to follow a \textbf{logarithmic trend}.
    
    Overall, the maximum values tend to be a bit more flat and unstable, while the average values show a smoother curve. The shapes are not as simple as the previous figures \ref{fig:average_bins_per_filtering} and \ref{fig:maximum_bins_per_filtering} since they do not follow a monotonic downward trend. For reasons unknown, the 0.25 $\sigma$ bar is the global minimum as we can observe an upward trend from 0.25 to 0.125 $\sigma$.

    Complexity values in 10 and 20 $\sigma$ are almost identical to the unfiltered setting (40 $\sigma$), followed by a gradual decrease that accelerates as the filtering becomes more aggressive until reaching the global minimum.

    We can observe the regression lines to understand this behavior better:

    \begin{figure}[H] 
        \centering
        \includegraphics[width=1.0\linewidth, height=0.4\textheight, keepaspectratio]{img/filtercomplexity_regression.png}
        \caption{Average and maximum number of complexity per filtering setting.}
    \end{figure}

    As expected, both average (red line) and maximum (green dashed line) complexity values follow a \textbf{logarithmic trend} ($R^2 = 0.014$ and $R^2 = 0.578$ respectively). The maximum fit is much better than the average fit due to the same reasons as explained in figure \ref{fig:regression_bins_per_filtering}. The maximum fit here is, however, much worse than the maximum fit in figure \ref{fig:regression_bins_per_filtering}, probably due to the unusual spike at 0.125 $\sigma$.

    Both curves show a downward trend as the filtering becomes more aggressive that accelerates the more we approach 0 $\sigma$, where all the values are removed. This acceleration is faster in the maximum complexity fit. 
    
    We also observe a very slight downward concavity in both curves approaching the unfiltered setting (40 $\sigma$) since the $x$ terms are slightly negative. For now, we are considering this an artifact of the regression that happens as a consequence of 40 $\sigma$ being an approximation of the unfiltered data even though it is very close.

    Also, it looks like using even less aggressive filtering settings (such as 30 $\sigma$) could be a better choice, but concluding this would require further experiments with more filtering settings between 20 $\sigma$ and the unfiltered data. Further experiments could also be conducted to understand the intriguing spike in complexity at 0.125 $\sigma$ and the behavior between it and totally filtered data (0 $\sigma$). Both of those points are left as future work.

    Analyzing the criteria together, we can conclude that the best filtering setting is \textbf{20 $\sigma$}, as it provides a significant reduction in the number of histogram bins while maintaining an almost identical complexity value compared to the unfiltered data. 
    
\section{Weight-type dimension}

    Following the methodology described in section \ref{sec:weight_type_analysis}, we obtained the plots for average and maximum LMC complexity values per weight-type combination across all models using the best filtering setting (20 $\sigma$):

    \begin{figure}[H]
        \centering
        \includegraphics[width=1.0\linewidth, height=0.5\textheight, keepaspectratio]{img/weight_average_complexity_bar.png}
        \caption{Average complexity per weight-type combination.}
        \label{fig:average_complexity_per_weight_type}
    \end{figure}

    \begin{figure}[H] 
        \centering
        \includegraphics[width=1.0\linewidth, height=0.5\textheight, keepaspectratio]{img/weight_maximum_complexity_bar.png}
        \caption{Maximum complexity per weight-type combination.}
        \label{fig:maximum_complexity_per_weight_type}
    \end{figure}

    Observing figures \ref{fig:average_complexity_per_weight_type} and \ref{fig:maximum_complexity_per_weight_type}, we can see that the shapes appear to be similar to each other. For a more careful analysis we will divide the ranking in multiple weight types (e.g. bias-embedding; norm-other) and single weight types (e.g. bias; other).

    We could not find a consistent pattern while analyzing the multiple weight types. It is obvious, however, that classes that include \textbf{other} have similar values since \textbf{other} weights are the majority of parameters in a model.

    Analyzing the single weight types, it is possible to see that while \textbf{norm} is responsible for the highest complexity values in any case and \textbf{embedding} is responsible for the lowest in any case, near zero. \textbf{Bias} and \textbf{other} weights vary between second and third place depending on the metric (average or maximum).

    It is unclear why this ranking occurs. As a consequence, it would be interesting to further investigate if training a model while optimizing the complexity value of certain weight types (e.g. norm) could lead to better performance or generalization. However, this is also left as future work.

    We will proceed with the analysis using all weight types combined except \textbf{embedding} (the \textbf{bias-norm-other} combination). This is justified as \textbf{embedding} has the lowest complexity values and is not likely to contribute significantly to the overall complexity of the model. Also because removing them allows us to follow the same methodology as the scaling laws study \cite{Kaplan2020}.

\section{Complexity vs Number of parameters}
\label{sec:complexity_vs_num_params_results}

    Following the methodology described in section \ref{sec:complexity_vs_num_params}, we obtained the following plot for average and maximum LMC complexity values against the number of parameters across the bias-norm-other weight-type combination using the best filtering setting (20 $\sigma$):

    \begin{figure}[H] 
        \centering
        \includegraphics[width=1.0\linewidth, height=0.8\textheight, keepaspectratio]{img/noembed-weights/param_average_complexity_histogram.png}
        \caption{Average complexity vs number of parameters.}
    \end{figure}

    A clear trend emerges from the histogram: as the number of parameters increases, the complexity values tend to stay mostly flat, with two sudden spikes between ranges $10^{9.742}$ - $10^{9.915}$ and $10^{10.298}$ - $10^{10.489}$ parameters.
    
    This indicates that complexity values tend to remain stable with the amount of parameters, except for certain model families in higher parameter count group that exhibit much higher complexity values than the rest.
    
    The reasons for the two spikes are unknown, but it is likely to be caused by the \textbf{GPT-OSS family} which is the common factor between both ranges and has noticeably higher complexity values.
    
    It is interesting to see that newer models with higher complexity values such as the \textbf{Phi-4 family}, the \textbf{LLaMA 4 family}, and the \textbf{GPT-OSS family} cluster themselves together in those two spikes. This is probably a coincidence since we can observe a gap between the two spikes with weaker models, but in any case more investigation is required.

    Also, since number of parameters is a known predictor of model performance \cite{Kaplan2020}, we can conclude right away that the high average complexity values found in the spike regions at the end, genuine or not, are likely to induce a bias towards better performance in the final analysis.

\section{Complexity vs Number of bins}

    Following the methodology described in section \ref{sec:complexity_vs_num_bins}, we obtained the following plot for average LMC complexity values against the number of histogram bins across the bias-norm-other weight-type combination using the best filtering setting (20 $\sigma$):

    \begin{figure}[H] 
        \centering
        \includegraphics[width=1.0\linewidth, height=0.8\textheight, keepaspectratio]{img/noembed-weights/bin_average_complexity_histogram.png}
        \caption{Average complexity vs number of histogram bins.}   
    \end{figure}

    Again, a clear trend emerges, we can see that values remain mostly flat with one continuous spike at the end between the range $10^{3.874}$ - $10^{3.922}$ bins. This spike is caused by the same models that caused the spikes in the previous section (\ref{sec:complexity_vs_num_params_results}): the \textbf{GPT-OSS family}, since in this case they are the only models in the spike range.

    The graphical similarity to the previous section (\ref{sec:complexity_vs_num_params_results}) might indicate that there may be a strong correlation between the number of parameters and the number of histogram bins. A possible explanation is that larger models tend to have a wider range of weight values, leading to more histogram bins when using the Freedman-Diaconis choice \cite{FreedmanDiaconis1981}. However, in this case, we don't have a gap between two spikes, which may indicate that this correlation is probably a coincidence.
    
\section{Complexity vs Benchmarks}
\label{sec:complexity_vs_benchmark_performance_results}

    Following the methodology described in section \ref{sec:complexity_vs_benchmark_performance}, using the bias-norm-other combination and the best filtering setting (20 $\sigma$) dataset, we made a control plot using a known correlation: parameter count vs benchmark performance. The results are as follows:

    \begin{figure}[H] 
        \centering
        \includegraphics[width=1.0\linewidth, height=0.4\textheight, keepaspectratio]{img/noembed-weights/control_correlation_comparison.png}
        \caption{Pearson correlation for parameter count vs benchmark performance.}
        \label{fig:control_correlation_comparison}
    \end{figure}

    \begin{figure}[H]
        \centering
        \includegraphics[width=1.0\linewidth, height=0.4\textheight, keepaspectratio]{img/noembed-weights/control_r2_linear_comparison.png}
        \caption{$R^2$ values for parameter count vs benchmark performance.}
        \label{fig:control_r2_linear_comparison}
    \end{figure}

    As expected, all the benchmarks show a positive correlation with parameter count, validating the control set. It is also noticeable that the linear regression $R^2$ values are relatively low indicating a non-linear relationship between parameter count and benchmark performance.

    The concatenation of all benchmarks (\textbf{all}) shows a correlation value of 0.5476 and an $R^2$ value of 0.3002, indicating a moderate positive correlation. This lies between the individual benchmarks.

    Now, we can analyze the actual relationship of interest: LMC complexity vs benchmark performance. The results are as follows:

    \begin{figure}[H] 
        \centering
        \includegraphics[width=1.0\linewidth, height=0.4\textheight, keepaspectratio]{img/noembed-weights/complexity_correlation_comparison.png}
        \caption{Pearson correlation for Complexity vs benchmark performance.}   
        \label{fig:complexity_correlation_comparison}
    \end{figure}

    \begin{figure}[H]
        \centering
        \includegraphics[width=1.0\linewidth, height=0.4\textheight, keepaspectratio]{img/noembed-weights/complexity_r2_linear_comparison.png}
        \caption{$R^2$ values for complexity vs benchmark performance.}
        \label{fig:complexity_r2_linear_comparison}
    \end{figure}

    It is also immediately clear that results are inconsistent across benchmarks. While \textbf{LMArena} and \textbf{MMLU} show a positive correlation between complexity and performance, \textbf{MMLU-Pro} and \textbf{OpenLLM} show a negative correlation.

    For individual benchmarks, the negative correlations have a smaller absolute value compared to the positive correlations, indicating a bias towards positive correlation. The correlation values are lower than the control's in all cases. The $R^2$ values are also lower than the control's in all cases, which might indicate an even less linear relationship between complexity and benchmark performance compared to control. It can also indicate that complexity is not a good predictor of benchmark performance.
    
    The \textbf{all} correlation, formed by analyzing a regression aggregating all benchmarks together and, as a consequence, with a bigger amount of data points, shows a positive correlation of 0.3066 and an $R^2$ value of 0.0940. Again, (\textbf{all}) is an intermediate between the individual benchmarks, both in correlation and $R^2$ values.

    While individual benchmarks show inconsistent correlations and score consistently worse than control and the aggregation of all benchmarks is also worse than control, having a 0.3066 general correlation is still pretty good. This suggests that there may be a general trend of increasing benchmark performance with increasing LMC complexity when considering a diverse set of benchmarks. Since benchmarks are an imperfect measure of model performance, it is possible that individual benchmark inconsistencies are smoothed out.

    We also found the statistical significance of the correlations found using t-tests \cite{Fisher1921}:
    
    \begin{table}[H]
        \centering
        \begin{tabular}{|c|c|c|c|c|c|}
            \hline
            \textbf{Benchmark} & \textbf{r} & \textbf{n} & \textbf{t} & \textbf{p-value} & \textbf{Significance} \\
            \hline
            LMArena & 0.3860 & 21 & 1.8239 & 0.0839 & No \\
            \hline
            MMLU & 0.4176 & 26 & 2.2515 & 0.0338 & Yes \\
            \hline
            MMLU-Pro & -0.1813 & 15 & -0.6647 & 0.5179 & No \\
            \hline
            OpenLLM & -0.2534 & 24 & -1.2287 & 0.2322 & No \\
            \hline
            all & 0.3066 & 86 & 2.9522 & 0.004089 & Yes \\
            \hline
        \end{tabular}
        \caption{Statistical significance of Pearson correlations between complexity and benchmark performance.}
        \label{tab:statistical_significance}
    \end{table}

    For individual benchmarks, it is possible to observe that only the \textbf{MMLU} correlation is statistically significant at a 0.05 significance level. While \textbf{LMArena} is not significant, it is really close. Still, the margins of error are quite high due to the small sample sizes (n $<$ 30), making it difficult to draw strong conclusions from the correlations found. 

    Interestingly, the benchmarks that are the furthest from significance are the ones with negative correlation (\textbf{MMLU-Pro} and \textbf{OpenLLM}), further indicating that there is more likely a positive bias towards correlation between complexity and benchmark performance than the other way around.

    For the aggregation of all benchmarks (\textbf{all}), the correlation is statistically significant by a wide margin, since it has a much larger sample size (n = 86) compared to individual benchmarks (n $<$ 30). If we only consider statistically significance as a criterion for confirming the initial hypothesis, it's safe to say we have strong evidence that LMC complexity generally correlates positively with benchmark performance.

    Before concluding, however, it is important observe the regression lines to understand the nature of the relationships found. We can look at the free and linear regression lines for the complexity plots:

    \begin{figure}[H] 
        \centering
        \includegraphics[width=1.0\linewidth, height=0.8\textheight, keepaspectratio]{img/noembed-weights/comp_lmarena_regression.png}
        \caption{LMC complexity vs LMArena benchmark.}
        \label{fig:comp_lmarena_regression}
    \end{figure}

    Here it is possible to see that the data points follow an almost perfect constant trend with two outliers that drive the positive correlation. This is likely related to the result seen in section \ref{sec:complexity_vs_num_params_results}, where complexity values remain mostly flat with a sudden spike at the end.

    We can see that those two outliers correspond to the \textbf{GPT-OSS family} models again and without them the correlation would be almost zero. Further investigation is required to understand why the complexity values of those models are so high and if they are genuine or artifacts.

    \begin{figure}[H] 
        \centering
        \includegraphics[width=1.0\linewidth, height=0.8\textheight, keepaspectratio]{img/noembed-weights/comp_mmlu_regression.png}
        \caption{LMC complexity vs MMLU benchmark.}
        \label{fig:comp_mmlu_regression}
    \end{figure}

    Again, a nearly constant trend is observed with a sudden spike at the end driving the positive correlation. Here we can see that the free regression fits an \textbf{exponential}, which can be the explanation for the apparently constant trend in both this and figure \ref{fig:comp_lmarena_regression} since exponentials can appear almost constant for small x values.

    But again, those outliers correspond to the \textbf{GPT-OSS family} models and without them the correlation would be almost zero, or even, as it appears, slightly negative.

    \begin{figure}[H] 
        \centering
        \includegraphics[width=1.0\linewidth, height=0.8\textheight, keepaspectratio]{img/noembed-weights/comp_mmlupro_regression.png}
        \caption{LMC complexity vs MMLU-Pro benchmark.}
    \end{figure}

    Here we see a different behavior: a slightly downward linear trend with high variability and two outliers in the middle driving an even more negative correlation.

    This time, those two outliers are not part of the \textbf{GPT-OSS family}, but from the \textbf{Gemma-2 family}. It is not clear why those models have such complexity values compared to the rest as other \textbf{Gemma} families seem to have a more normal behavior, in any case, further investigation is required.

    \begin{figure}[H] 
        \centering
        \includegraphics[width=1.0\linewidth, height=0.8\textheight, keepaspectratio]{img/noembed-weights/comp_openllm_regression.png}
        \caption{LMC complexity vs OpenLLM benchmark.}
    \end{figure}

    And here the most interesting behavior is observed: two lines are formed, one with a slight logarithmic upward trend and another with a slight logarithmic downward trend, both regression fits trying to find a middle ground between them.

    The downward line is formed by the \textbf{Gemma-2 family}, \textbf{Phi-1\_5 family}, \textbf{Phi-2 family} and \textbf{GPT-2 family}.

    The upward line is formed by the \textbf{LLaMA-4 family}, \textbf{LLaMA-3 family}, \textbf{LLaMA-2 family}, \textbf{Phi-4 family}, \textbf{GPT-OSS family}, \textbf{Gemma family}, bigger parameter count models from \textbf{Gemma-2 family},
    and \textbf{Gemma-3 family} models.

    Appears that there is no clear reason why those two lines are formed: \textbf{parameters count} does not seem to be a factor since the upward line has both higher and lower parameter count models. The downward line, however, appears to be formed mostly by older models with lower parameters count. \textbf{Family type} also does not seem to be a factor since both lines have multiple different families. Maybe a good guess would be something in common in the \textbf{architectures}, but further investigation is required.

    There was one outlier with a very high complexity value for its benchmark performance: the \textbf{GPT-2} model. It is also not clear why.

    We can finally look at the aggregation of all benchmarks:

    \begin{figure}[H] 
        \centering
        \includegraphics[width=1.0\linewidth, height=0.8\textheight, keepaspectratio]{img/noembed-weights/comp_all_regression.png}
        \caption{LMC complexity vs all benchmarks aggregated.}
    \end{figure}

    It seems to follow the same pattern as the first two individual benchmarks (figures \ref{fig:comp_lmarena_regression} and \ref{fig:comp_mmlu_regression}): an almost constant trend with a sudden spike at the end driving the positive correlation. The spike is again caused by the \textbf{GPT-OSS family} models and without them the correlation would be almost zero. A few models seem to form another almost constant line below the main one, with a small downward trend.

    We can roughly classify the benchmarks based on their regression plots:
    \begin{itemize}
        \item Constant + outlier's upward trend: \textbf{LMArena}, \textbf{MMLU}, \textbf{all};
        \item General downward trend + outlier's downward trend: \textbf{MMLU-Pro};
        \item Two opposite trends: \textbf{OpenLLM};
    \end{itemize}

    \textbf{All} was put in the first category since the influence of the first category benchmarks made it follow the same trend approximately. 
    
    Also, it is interesting to see that \textbf{MMLU} and \textbf{MMLU-Pro}, which are very similar benchmarks, behaved quite differently. It is noticeable that \textbf{MMLU-Pro} has a bigger concentration of newer models which might indicate the reason, but still further investigation is required.

    In all cases, we can see that the free regressions do not appear to be able to capture the behavior of the data points well due to their outliers. The linear regressions are also influenced by the same factor, leading to low $R^2$ values. This is a further indicator that LMC complexity appears not to be a reliable predictor of benchmark performance.

    As a bonus, for the sake of completeness, we made a bar plot ranking the top 20 configurations (model + weight-type combination) sorted by highest correlation between LMC complexity and benchmark performance (all benchmarks aggregated):

    \begin{figure}[H] 
        \centering
        \includegraphics[width=1.0\linewidth, height=0.8\textheight, keepaspectratio]{img/all-weights/top_20_correlations.png}
        \caption{Top 20 configurations by Pearson correlation.}
    \end{figure}

    We can see that the top 20 correlations are mixed between negative and positive biggest absolute values. The top 5 is all positive.
    
    The ranking is dominated by very high filtering and, interestingl, the top 1 is 0.25 $\sigma$, which is part of the global minimum complexity value of the filtering dimension analysis.

    Except for the \textbf{MMLU}, all the benchmarks appeared. Almost all of the weight-type combinations were \textbf{bias} or included it, the only exceptions were 2 "\textbf{embedding-norm}".
    
