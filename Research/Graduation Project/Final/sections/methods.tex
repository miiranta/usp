\section{Methodology}

\subsection{Testing Environment}

\subsection{Model Selection}

Due to the wide availability of open models combined with the ease of accessing them through its Transformers library \cite{Wolf2019}, \textbf{Hugging Face} was chosen as the source for model selection in this study.

Hugging Face is a platform that hosts a variety of machine learning models, datasets, and tools. It is widely used in the AI research community for sharing and collaborating on machine learning projects \cite{HuggingFaceResearchGate, HuggingFaceMain}. Other platforms such as Ollama \cite{Ollama} were considered, but ultimately not chosen due to the limited number of models and lack of company diversity. 

Model selection proceeded in two stages:

\begin{enumerate}
    \item First, we identified major technology companies by market capitalization that publish openly released language models. The companies considered were \textbf{OpenAI}, \textbf{Google}, \textbf{Meta} and \textbf{Microsoft}.
    \item Second, for each company we compiled a candidate set consisting of every model that satisfied the following criteria:
    
        \begin{itemize}
            \item Must be available on the Hugging Face platform on the official company account.
            \item The model is a transformer-based language model.
            \item Model weights are publicly accessible (open weights), including models released behind
            gated access.
            \item The model is text-only (no multimodal image/audio inputs), .
            \item The model is an original base model rather than a task-specific fine-tuned variant.
            \item The total parameter count is below 150 billion. This upper bound was imposed due to
            hardware and inference limitations in the computational environment used for our experiments.
            \item The model is supported by the Hugging Face Transformers library \cite{Wolf2019} (i.e., it can be
            instantiated via the AutoModel/AutoTokenizer utilities), which ensures consistent loading and
            preprocessing across the candidate set.
            \item There's at least one publicly available benchmark result for the model among the selected benchmarks (section \ref{sec:benchmark_selection}).
        \end{itemize}

\end{enumerate}

The final selection of models used in this study is listed in Table \ref{tab:selected_models}.

\begin{table}[H]
    \begin{tabular}{p{0.24\linewidth} p{0.24\linewidth} p{0.24\linewidth} p{0.24\linewidth}}
        \hline
        Meta & Google & Microsoft & OpenAI \\
        \hline
        meta-llama/Llama-4-Scout-17B-16E & google/gemma-3-27b-pt & microsoft/Phi-4-mini-reasoning & openai/gpt-oss-120b \\
        meta-llama/Llama-3.2-3B & google/gemma-3-12b-pt & microsoft/Phi-4-reasoning & openai/gpt-oss-20b \\
        meta-llama/Llama-3.2-1B & google/gemma-3-4b-pt & microsoft/Phi-4-reasoning-plus & openai-community/gpt2-xl \\
        meta-llama/Llama-3.1-70B & google/gemma-3-1b-pt & microsoft/phi-4 & openai-community/gpt2-large \\
        meta-llama/Llama-3.1-8B & google/gemma-3-270m & microsoft/phi-2 & openai-community/gpt2-medium \\
        meta-llama/Meta-Llama-3-70B & google/gemma-2-27b & microsoft/phi-1\_5 & openai-community/gpt2 \\
        meta-llama/Meta-Llama-3-8B & google/gemma-2-9b & microsoft/phi-1 & \\
        meta-llama/Llama-2-70b-hf & google/gemma-2-2b &  & \\
        meta-llama/Llama-2-13b-hf & google/gemma-7b &  & \\
        meta-llama/Llama-2-7b-hf & google/gemma-2b &  & \\
            & google/recurrentgemma-9b &  & \\
            & google/recurrentgemma-2b &  & \\
        \hline
    \end{tabular}
    \caption{Selected language models included in this study.}
    \label{tab:selected_models}
\end{table}

\begin{tcolorbox}[colback=gray!10, colframe=black, boxrule=1pt, rounded corners]
    \textbf{Meta Models}:
    \cite{MetaLlama4Scout}
    \cite{MetaLlama3.2-3B}
    \cite{MetaLlama3.2-1B}
    \cite{MetaLlama3.1-70B}
    \cite{MetaLlama3.1-8B}
    \cite{MetaLlama3-70B}
    \cite{MetaLlama3-8B}
    \cite{MetaLlama2-70b}
    \cite{MetaLlama2-13b}
    \cite{MetaLlama2-7b}

    \textbf{Google Models}:
    \cite{GoogleGemma3-27b}
    \cite{GoogleGemma3-12b}
    \cite{GoogleGemma3-4b}
    \cite{GoogleGemma3-1b}
    \cite{GoogleGemma2-27b}
    \cite{GoogleGemma2-9b}
    \cite{GoogleGemma2-2b}
    \cite{GoogleGemma-7b}
    \cite{GoogleGemma-2b}
    \cite{GoogleRecurrentGemma-9b}
    \cite{GoogleRecurrentGemma-2b}

    \textbf{Microsoft Models}:
    \cite{MicrosoftPhi4-mini-reasoning}
    \cite{MicrosoftPhi4-reasoning}
    \cite{MicrosoftPhi4-reasoning-plus}
    \cite{MicrosoftPhi4}
    \cite{MicrosoftPhi2}
    \cite{MicrosoftPhi1-5}
    \cite{MicrosoftPhi1}

    \textbf{OpenAI Models}:
    \cite{OpenAIGPT-oss-120b}
    \cite{OpenAIGPT-oss-20b}
    \cite{OpenAIGPT2-xl}
    \cite{OpenAIGPT2-large}
    \cite{OpenAIGPT2-medium}
    \cite{OpenAIGPT2}
\end{tcolorbox}

\subsection{LMC Complexity}

    According to \cite{LopezRuiz1995}, LMC Statistical Complexity is the product of two other measures: Disequilibrium and Shannon entropy. It captures both the structured and unstructured aspects of the distribution:
    \[ C_{LMC} = H \times D \]

    Disequilibrium measures how far a probability distribution is from being uniform, quantifying the "order" or structure in the data. It is calculated as:
    \[ D = \sum_{i=1}^{n} \left(p_i - \frac{1}{n}\right)^2 \]

    Shannon entropy measures the amount of uncertainty or randomness in a probability distribution. The normalized Shannon entropy is given by:
    \[ H = -K \sum_{i=1}^{n} p_i \log p_i \]

    The values \( p_i \) represent the probabilities associated with each state \( i \) in the distribution, and \( n \) is the total number of states. K is a positive constant and, in our case, is set to 1 for simplicity. K can be changed later since \( C_{LMC} = (-K \sum_{i=1}^{n} p_i \log p_i)  \times D \) is equivalent to \( C_{LMC} = K \times (- \sum_{i=1}^{n} p_i \log p_i)  \times D \).

    \subsubsection{Data Discretization}

        To compute the LMC complexity of a finite array of floating-point numbers, we first construct a histogram to discretize the data into a probability distribution. That's justified as the chance of finding two exact numbers is extremely low and, as a consequence, it is hard to determine the probability of each value. 
        
        We will call the set of all data points as \( S \), the total amount of data points as \( N \) and the number of bins they will be distributed into as \( n \). The probabilities \( p_i \) are then calculated as \( p_i = \frac{f_i}{N} \) where \( f_i \) is the frequency count of data points in bin \( i \).

        As expected, this approach revisits a classic issue in histogram-based analysis: the choice of the number of bins \( n \) will impact the resulting probability distribution, which is specially problematic to the LMC complexity measure; variations in \( n \) can cause significant fluctuations in the final result. Selecting an inappropriate amount may produce misleading values, either by oversimplifying the distribution (too few bins) or by introducing noise (too many bins). 

        There are a variety of methods to determine \( n \) in a histogram, most of them with their own advantages and disadvantages. Commonly used methods such as \textbf{Sturges' formula} \cite{Sturges1926} and \textbf{Rice Rule} \cite{RiceRule} rely only on the number of data points, while others like \textbf{Scott's normal reference rule} and \textbf{Freedman-Diaconis' choice} also take into account the data distribution by using standard deviation and interquartile range, respectively \cite{Knuth2006}.   

        We chose to use the \textbf{Freedman-Diaconis' choice} as it adapts better to our needs. This is justified since \( N \) often consists of billions of numbers, and the distribution, although mostly concentrated between -1 and 1, can become sparse due to outliers and, as a consequence, require a larger number of bins to capture its characteristics accurately. The Freedman-Diaconis rule helps mitigate the influence of outliers by using the interquartile range \( IQR \) to determine bin width \( h \). The rule is defined as follows \cite{FreedmanDiaconis1981}:

        \[ h = \frac{2 \times IQR}{N^{1/3}} \]

        where IQR is the interquartile range of the data which is calculated as \( Q3 - Q1 \), \( Q3 \) and \( Q1 \) are the values at the 75th and 25th percentiles in the data, respectively. Since \( N \) is very large, the percentiles are computed based on a random sample of 100000 data points to reduce computational cost. The sample size was determined empirically to be large enough to provide stable estimates of the percentiles, the final number of bins showed no variance between tests.

        Finally, the number of bins \( n \) can then be calculated as:

        \[ n = \frac{\max(S) - \min(S)}{h} \]

    \subsubsection{Complexity Measure Extraction}

\subsection{Inference Capability}

    Often called model performance, inference capability refers to how well a trained neural network performs on unseen data. This is typically measured using various metrics depending on the specific task the model is designed for. 
    
    In the previously cited research paper by OpenAI \cite{Kaplan2020}, performance was associated with test cross-entropy loss. This metric quantifies the difference between the predicted probability distribution output by the model and the true distribution of the target labels. Lower cross-entropy loss values indicate better model performance, as they reflect a closer alignment between predictions and actual outcomes.

    Unfortunately, not all models we intend to analyze have publicly available training and test data. It's also not possible to run models in a training setting due to performance limitations of the computational environment available. Therefore, we will have to rely on imperfect proxies for performance such as \textbf{benchmarks}.

    \subsubsection{Benchmark Selection}
    \label{sec:benchmark_selection}

        Benchmarks are standardized tests designed to evaluate the performance of machine learning models across various tasks. They provide a common ground for comparison by measuring how well different models perform on the same datasets using predefined metrics. 
        
        According to \cite{Owen2024}, benchmarks such as the famous MMLU, correlate fairly well with the predicted test loss determined by scaling laws, making them suitable proxies.

        They are, however, not perfect. Some benchmarks may fail to capture all aspects of a model's capabilities, leading to an incomplete assessment of performance. Other problems such as Data Contamination \cite{Magar2022} can influence the validity of results and makes it hard to fairly compare models made in different times. It is also hard to find benchmarks that are widely reported for all models we intend to analyze.

        A nice proposal for future research to avoid the problems cited above would be training a model from scratch twice: optimizing it initially for minimal loss and then for minimal loss + maximum LMC complexity, then comparing the results. This is however out of the scope of this work.

        The benchmark selection followed three main heuristics:
        \begin{itemize}
            \item \textbf{Relevance}: The benchmark should be widely recognized and accepted in the machine learning community, e.g., used in major research papers.
            \item \textbf{Generality}: It should cover a range of tasks and data types to provide an assessment of model performance across different scenarios, that is, not being specialized in one task.
            \item \textbf{Availability}: The benchmark results should be publicly available. Benchmarks that cover more of the selected models were preferred.
        \end{itemize}

        Based on those heuristics, the benchmarks selected were:
        \begin{itemize}
            \item \textbf{MMLU (5-shot)}: Massive Multitask Language Understanding, a benchmark that tests models across 57 tasks spanning various subjects and difficulty levels. Widely used to evaluate LLMs \cite{Hendrycks2020}.

            \item \textbf{MMLU-Pro (5-shot)}: An enhanced version of MMLU that includes additional tasks and updated datasets to provide a better evaluation \cite{MMPro2024}.
            
            \item \textbf{OpenLLM (Average)}: A benchmark suite that evaluates models on a variety of tasks, including language understanding, generation, and reasoning. It aggregates results from multiple datasets to provide an overall performance score \cite{Myrzakhan2024}.
            
            \item \textbf{LMArena (Score)}: An online platform where users can submit questions and receive responses from two anonymous large language models (LLMs), then vote on which answer they prefer, helping to crowdsource human preferences for evaluating and ranking LLMs. Its evaluation works by collecting thousands of pairwise comparisons users, using  the Bradley-Terry system to estimate win rates and compute rankings/scores. \cite{Chiang2024}.
        \end{itemize}

    \subsubsection{Benchmark Measure Extraction}

        Benchmark values were manually collected from multiple sources, from higher priority to lower priority: (1) official \textit{Hugging Face} model pages, (2) original research papers, (3) official websites, (4) third-party websites. If more than one source was available for the same model and benchmark, the one with higher priority was chosen.
        
        \begin{tcolorbox}[colback=gray!10, colframe=black, boxrule=1pt, rounded corners]
            \textbf{Meta Models}: 
            \cite{MetaLlama4Scout}, 
            \cite{MetaLlama3.2-3B}, 
            \cite{MetaLlama3.2-1B}, 
            \cite{MetaLlama3.1-70B}, 
            \cite{MetaLlama3.1-8B}, 
            \cite{MetaLlama3-70B}, 
            \cite{MetaLlama3-8B}, 
            \cite{MetaLlama2-70b}, 
            \cite{MetaLlama2-13b}, 
            \cite{MetaLlama2-7b}
            
            \textbf{Google Models}: 
            \cite{GoogleGemma3-27b}, 
            \cite{GoogleGemma3-12b}, 
            \cite{GoogleGemma3-4b}, 
            \cite{GoogleGemma3-1b}, 
            \cite{GoogleGemma2-27b}, 
            \cite{GoogleGemma2-9b}, 
            \cite{GoogleGemma2-2b}, 
            \cite{GoogleGemma-7b}, 
            \cite{GoogleGemma-2b}, 
            \cite{GoogleRecurrentGemma-9b}, 
            \cite{GoogleRecurrentGemma-2b}
            
            \textbf{Microsoft Models}: 
            \cite{MicrosoftPhi4-mini-reasoning}, 
            \cite{MicrosoftPhi4-reasoning}, 
            \cite{MicrosoftPhi4-reasoning-plus}, 
            \cite{MicrosoftPhi4}, 
            \cite{MicrosoftPhi2}, 
            \cite{MicrosoftPhi1-5}, 
            \cite{MicrosoftPhi1}, 
            \cite{Phi1.5}, 
            \cite{Phi2Blog}
            
            \textbf{OpenAI Models}: 
            \cite{OpenAIGPT-oss-120b}, 
            \cite{OpenAIGPT-oss-20b}, 
            \cite{OpenAIGPT2-xl}, 
            \cite{OpenAIGPT2-large}, 
            \cite{OpenAIGPT2-medium}, 
            \cite{OpenAIGPT2}, 
            \cite{OpenAIOpenModels},
            
            \textbf{Additional References}: 
            \cite{HuggingFaceMain}, 
            \cite{Ollama},
            \cite{LLMExplorer}, 
            \cite{RankedAGI},
        \end{tcolorbox} 

        Even with all those sources, not all models had results available for all benchmarks. Figure \ref{fig:benchmark_availability} shows the availability matrix.

        \begin{figure}[H]
            \includegraphics[width=\linewidth]{img/benchmark_availability.png}
            \caption{Availability of benchmark results for the selected models.}
            \label{fig:benchmark_availability}
        \end{figure}

\subsection{Comparing LMC Complexity and Inference Capability}


