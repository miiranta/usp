\section{Methodology}

\subsection{Creating the Phrase Dataset}


    \subsubsection{Scraping}

        We collected COPOM (Central Bank of Brazil's Monetary Policy Committee) minutes using Python and Selenium from the official listing~\cite{BCB_COPOM_Minutes}, downloading both HTML and PDF versions when available.

        The dataset \(C\) contains 251 COPOM minutes from January 1996 to July 2025. Each minute \(c\) has an associated date \(d_i\) and may have HTML and/or PDF versions.


    \subsubsection{Parsing}

        \textbf{For each} COPOM minute \(c\) in \(C\):
        \begin{enumerate}

            \item{Type-Specific Pre-Processing}

                HTML: extract body content, remove formatting tags (\texttt{strong}, \texttt{i}, \texttt{br}) while preserving inner content, remove other tags with content.

                PDF: extract phrases using SpaCyLayout~\cite{neumann2019scispacy} with \texttt{pt\_core\_news\_lg} model.

                We create phrase lists \(P_{c}^{\text{html}}\) and \(P_{c}^{\text{pdf}}\), each containing phrases from respective versions.

            \item{General Pre-Processing}
                
                For each phrase: (1) Remove newlines and tabs; (2) Remove tag entities (e.g., \texttt{\&nbsp}); (3) Reduce consecutive spaces, commas, periods to single characters; (4) Add period at end if missing.

            \item{Length Filtering}

                Discard single-word phrases and phrases with character count below \(\mu\), the mean character count from the respective source \(P_{c}^{\text{x}}\).

            \item{Blacklist Filtering}

                Remove phrases containing: (1) \textit{javascript}; (2) \textit{cookies}; (3) \textit{expand\_less}; (4) \textit{content\_copy}; or (5) \textit{Garantir a estabilidade do poder de compra da moeda}. 
                
                While terms (1) to (4) are related to web page elements and scripts, term (5) is the Brazilian Central Bank's motto, which often appears in the minutes and is not relevant for sentiment analysis.

        \end{enumerate}

        Finally, we compare phrase counts between sets and select the one with more phrases (PDF if equal to ensure superior quality), creating the selected set \(F_{d_i}\) for each date \(d_i\). The set \(F\) contains all sets \(F_{d_i}\).

    \subsubsection{Phrase Selection}

        We flatten \(F\) into list \(L\) of tuples (\text{phrase}, \text{date}).

        We perform dense passage retrieval using semantic similarity filtering. We compute embeddings with \textbf{Qwen3-Embedding-0.6B}~\cite{zhang2025qwen3embedding} and retain phrases with cosine similarity > 0.6 to ``inflation''. We use PyTorch for GPU acceleration, pandas for manipulation, and scikit-learn for similarity.
        
        The final dataset \(F^{infl}\) contains 9,378 inflation-related phrases across 251 dates (~37.4 phrases per date).



\subsection{Creating the Sentiment Datasets}


    \subsubsection{LLM Evaluation Dataset}
    \label{sec:llm_evaluation_dataset}

        We evaluated phrase sentiment using nine LLMs from different companies:
        \begin{enumerate}[noitemsep]
            \item \textit{openai/gpt-5}
            \item \textit{anthropic/claude-sonnet-4}
            \item \textit{google/gemini-2.5-pro}
            \item \textit{x-ai/grok-4-fast}
            \item \textit{openai/gpt-oss-120b}
            \item \textit{meta-llama/llama-4-maverick}
            \item \textit{google/gemma-3-27b-it}
            \item \textit{microsoft/phi-4}
            \item \textit{deepseek/deepseek-chat-v3.1}
        \end{enumerate}

        \textbf{For each model}, we made one independent request per phrase in \(F^{infl}\), without prior context.

        The prompt, formulated in Brazilian Portuguese by economist Cézio Luiz Ferreira Junior, explained the task and appended the phrase:

        \begin{quote}
            \noindent\textbf{DEFINIÇÃO DE OTIMISMO:}
            Ocorre quando as projeções indicam que a inflação ficará abaixo da meta ou dentro do intervalo de tolerância com folga.
            Isso pode sinalizar que o Banco Central vê espaço para reduzir juros ou manter uma política monetária mais acomodatícia.

            \textbf{DEFINIÇÃO DE PESSIMISMO:}
            Ocorre quando as projeções apontam para inflação acima da meta ou próxima do teto do intervalo de tolerância.
            Isso sugere preocupação com pressões inflacionárias e pode justificar uma política monetária mais restritiva.

            \textbf{AVALIE A FRASE COMO:}
            O para OTIMISTA, N para NEUTRA, P para PESSIMISTA.
            SUA RESPOSTA DEVE SER APENAS UMA LETRA, SEM QUALQUER OUTRO TEXTO.

            \textbf{FRASE A SER AVALIADA:}
            <<<<PHRASE>>>>
        \end{quote}

        Models classify phrases as optimistic, neutral, or pessimistic. Responses (O, N, P) are converted to 1, 0, -1; unparseable responses labeled -2 (rare occasions).

        We use OpenRouter API for unified access. We determine token limits by testing on the first date's phrases; if any receives -2, we double the limit and repeat testing until all responses are successful. Table \ref{tab:token_limits} shows the final token limits used.

        \begin{table}[h]
            \centering
            \begin{tabular}{|l|r|}
                \hline
                \textbf{Model} & \textbf{Token Limit} \\
                \hline
                openai/gpt-5 & 1024 \\
                openai/gpt-oss-120b & 512 \\
                google/gemini-2.5-pro & 128 \\
                google/gemma-3-27b-it & 8 \\
                deepseek/deepseek-chat-v3.1 & 4 \\
                others & 1 \\
                \hline
            \end{tabular}
            \caption{Token limits per LLM model.}
            \label{tab:token_limits}
        \end{table}

        We discarded evaluations not equal to 1 or -1. We concatenated results into sets \(E_{m}\) for each model, containing tuples (\text{phrase}, \text{date}, \text{sentiment}). The set \(E_{Models}\) contains all \(E_{m}\).



\subsection{Human Evaluation Dataset}

    We created three human evaluation datasets:
    \begin{enumerate}
        
        \item \textit{Open}
        
        A website with O/N/P selection for randomly selected phrases from \(F^{infl}\), limited to 10 phrases per browser per 24h. Distributed to economics graduate students at USP and Unicamp. Result: \(E_{Open}\) with 278 tuples.
        
        \item \textit{Specialist}
        
        A subset \(F^{infl-350}\) of 350 random phrases from \(F^{infl}\), with date labels Base64-encoded to prevent bias. Labeled by economist Cézio Luiz Ferreira Junior as: 1 (optimistic), 0 (neutral), -1 (pessimistic), -2 (non-related), -3 (not understood). Result: \(E_{Specialist}\) with 350 tuples.

        \item \textit{Consolidated}
        
        \(F^{infl-350}\) re-analyzed by the specialist and two additional professors together, discussing each phrase to reach consensus. Result: \(E_{Consolidated}\) with 220 tuples.

    \end{enumerate}

    Again, we discarded evaluations not equal to 1 or -1. for all methods. Set \(E_{Humans}\) contains all \(E_{h}\).



\subsection{Testing Inflation Prediction Performance}

    We test two common inflation prediction models: (1) \textbf{ARIMA} and (2) \textbf{LSTM}. 
    
    The goal is to check whether adding sentiment variables from LLM evaluations reduces RMSE compared to historical inflation data alone, and whether bias correction from human evaluations further improves performance.

    \subsubsection{Creating the Input Datasets}

        \textbf{For each} set of the power set of \(E_{Models}\), except for the empty one, we will concatenate the tuples of the selected \(E_{m}\) sets into a single set named \(U_{i}\). 

        \textbf{For each} \(U_{i}\) created, we will create \(j\) more tuples in the form (\(U_{i}\), \(V_{j}\)), where \(V_{j}\) is one of the three human evaluation datasets in \(E_{Humans}\).

        \textbf{For each} tuple (\(U_{i}\), \(V_{j}\)) created, we will create \(k\) more tuples in the form (\(U_{i}\), \(V_{j}\), \(eq_{k}\)), where \(eq_{k}\) is one of the equations to be used for bias correction later.

        The tuple (\(U_{i}\), \(V_{j}\), \(eq_{k}\)) represents the sentiment evaluations from the selected LLM models combined with the human evaluation dataset \(V_{j}\) for bias correction using equation \(eq_{k}\).

        The possible equation forms for \(eq_{k}\) are: linear (\(x + a\)), affine (\(bx + a\)), quadratic (\(cx^2 + bx + a\)), and cubic (\(dx^3 + cx^2 + bx + a\)).

        \textbf{For each} tuple (\(U_{i}\), \(V_{j}\), \(eq_{k}\)), we create three different input datasets for inflation prediction models, each one of them will provide a list of tuples in the form of (\({Inflation}, {Sentiment}\)):

        \begin{enumerate}
            \item \textit{Only Inflation (Baseline)} 
            
            IPCA monthly (Series 433)~\cite{BCB_IPCA_API}.

            The sentiment variable is set to 0 for associated inflation values.

            \item \textit{Inflation + Sentiment (Without Correction)}
            
            IPCA monthly (Series 433) + Sentiment variable created as an average grade per date of the evaluations in \(U_{i}\) (interpolated by cubic spline and fitted to the available IPCA dates)

            \item \textit{Inflation + Sentiment (With Correction)}
            
            IPCA monthly (Series 433) + Sentiment variable created as an average grade per date of the evaluations in \(U_{i}\) (interpolated by cubic spline and fitted to the available IPCA dates) corrected based on the bias measured from \(V_{j}\).
            
            The correction process works as follows: 
            
            First, both LLM sentiment scores from \(U_{i}\) and human evaluations from \(V_{j}\) are averaged by date and interpolated using cubic spline to create continuous daily time series. 
            
            Then, we find a single set of parameters of the transformation equation \(eq_{k}\) that when applied to all dates individually minimizes the mean squared error (MSE).
            
            The equation is applied per date with the variable \(x\) representing the average LLM sentiment score in that date, and the resulting value representing the bias-corrected sentiment score.
            
            The optimization uses gradient descent with the Adam optimizer~\cite{kingma2014adam} (1000 epochs, learning rate 0.01) implemented in PyTorch. 
            
            These optimized parameters are then applied to the equation to transform the LLM sentiment score for each individual date in \(U_{i}\), producing bias-corrected values aligned with human judgment from \(V_{j}\).

        \end{enumerate}

        Finally, for each tuple (\(U_{i}\), \(V_{j}\), \(eq_{k}\)) created, we have 3 new associated lists of tuples in the form of (\({Inflation}, {Sentiment}\)), each called \(IN_{ijkm}\) where \(i\) is the LLM model combination used; \(j\) is the human evaluation dataset used for bias correction; \(k\) is the equation type used for bias correction; and \(m\) \(\in\) \{\textit{Baseline}, \textit{Without Correction}, or \textit{With Correction}.\}

        The set \(IN\) contains all sets \(IN_{ijkm}\).


    \subsubsection{Running the Tests}

        Looking at the \(IN\), we see that this approach involves repetition of \(IN_{ijkm}\) datasets since, for example, \textit{Baseline} is the same for all tuples (\(U_{i}\), \(V_{j}\), \(eq_{k}\)).

        While this is bad from a computational efficiency perspective, it provides a control for every experiment: \textit{Baseline} should be a control \textit{Without Correction} and \textit{With Correction}, while \textit{Without Correction} should be a control for \textit{With Correction}.

        \textbf{For each} \(IN_{ijkm}\) in \(IN\), we run both ARIMA and LSTM~\cite{hochreiter1997lstm} inflation prediction models on the respective dataset with a 70/30 train/test split.

        We employ ARIMA with sentiment as exogenous variable~\cite{moslemi2024comprehensive} using walk-forward validation, and LSTM with 5000 neurons trained with NAdam optimizer~\cite{dozat2016nadam} (learning rate 0.001, max 10,000 epochs, early stopping patience 10). The highly parameterized LSTM aligns with recent insights on double descent~\cite{schaeffer2023double}, where increased complexity improves generalization in the overparameterized regime. Both models are evaluated using Root Mean Squared Error (RMSE).

        In total, we conducted 36,792 tests: $(2^9 - 1)$ LLM combinations $\times$ 3 human datasets $\times$ 4 equation types $\times$ 3 dataset types $\times$ 2 models.


    \subsubsection{Statistical Significance Testing}

        To assess whether the observed RMSE improvements are statistically significant, we perform a one-sample t-test on the percentage improvements relative to baseline. For each model and correction type, we test whether the mean improvement percentage differs significantly from zero (no improvement).

        We grouped all \(IN_{ijkm}\) instances by model type (LSTM or ARIMA) and the \(m\) index value (\textit{Without Correction} or \textit{With Correction}), excluding \textit{Baseline} since it serves as the reference point. For each group, we calculated the percentage improvement by comparing the RMSE of each \(IN_{ijkm}\) against its corresponding baseline: \(\text{Improvement}_{\%} = \frac{\text{RMSE}_{\text{baseline}} - \text{RMSE}_{ijkm}}{\text{RMSE}_{\text{baseline}}} \times 100\). This yields four distinct groups for analysis: LSTM-Uncorrected, LSTM-Corrected, ARIMA-Uncorrected, and ARIMA-Corrected. Each group aggregates results across all LLM combinations (\(i\)), human evaluation datasets (\(j\)), and equation types (\(k\)), resulting in $n = 6{,}132$ observations per group (511 LLM combinations $\times$ 3 human datasets $\times$ 4 equation types).

        The null hypothesis $H_0: \mu = 0$ states that sentiment inclusion provides no average improvement, tested against the alternative $H_1: \mu \neq 0$ (nonzero improvement). The t-statistic is:

        \[
        t = \frac{\bar{x}}{s / \sqrt{n}}
        \]

        where $\bar{x}$ is the mean improvement percentage across all experimental runs, $s$ is the sample standard deviation, and $n$ is the sample size. The p-value indicates the probability of observing such improvements if sentiment truly had no effect. We consider $p < 0.001$ as highly significant, $p < 0.01$ as very significant, and $p < 0.05$ as significant. 

